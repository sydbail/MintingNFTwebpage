{"ast":null,"code":"'use strict';\n\nvar _asyncToGenerator = require(\"/Users/sydneybailey/Internship/contract-testing/node_modules/@babel/runtime/helpers/asyncToGenerator\").default;\n\nconst dagPb = require('@ipld/dag-pb');\n\nconst {\n  Bucket,\n  createHAMT\n} = require('hamt-sharding');\n\nconst DirSharded = require('./dir-sharded');\n\nconst log = require('debug')('ipfs:mfs:core:utils:hamt-utils');\n\nconst {\n  UnixFS\n} = require('ipfs-unixfs');\n\nconst last = require('it-last');\n\nconst {\n  CID\n} = require('multiformats/cid');\n\nconst {\n  hamtHashCode,\n  hamtHashFn,\n  hamtBucketBits\n} = require('./hamt-constants');\n/**\n * @typedef {import('multiformats/cid').CIDVersion} CIDVersion\n * @typedef {import('ipfs-unixfs').Mtime} Mtime\n * @typedef {import('../').MfsContext} MfsContext\n * @typedef {import('@ipld/dag-pb').PBNode} PBNode\n * @typedef {import('@ipld/dag-pb').PBLink} PBLink\n */\n\n/**\n * @param {MfsContext} context\n * @param {PBLink[]} links\n * @param {Bucket<any>} bucket\n * @param {object} options\n * @param {PBNode} options.parent\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n * @param {string} options.hashAlg\n */\n\n\nconst updateHamtDirectory = /*#__PURE__*/function () {\n  var _ref = _asyncToGenerator(function* (context, links, bucket, options) {\n    if (!options.parent.Data) {\n      throw new Error('Could not update HAMT directory because parent had no data');\n    } // update parent with new bit field\n\n\n    const data = Uint8Array.from(bucket._children.bitField().reverse());\n    const node = UnixFS.unmarshal(options.parent.Data);\n    const dir = new UnixFS({\n      type: 'hamt-sharded-directory',\n      data,\n      fanout: bucket.tableSize(),\n      hashType: hamtHashCode,\n      mode: node.mode,\n      mtime: node.mtime\n    });\n    const hasher = yield context.hashers.getHasher(options.hashAlg);\n    const parent = {\n      Data: dir.marshal(),\n      Links: links.sort((a, b) => (a.Name || '').localeCompare(b.Name || ''))\n    };\n    const buf = dagPb.encode(parent);\n    const hash = yield hasher.digest(buf);\n    const cid = CID.create(options.cidVersion, dagPb.code, hash);\n\n    if (options.flush) {\n      yield context.repo.blocks.put(cid, buf);\n    }\n\n    return {\n      node: parent,\n      cid,\n      size: links.reduce((sum, link) => sum + (link.Tsize || 0), buf.length)\n    };\n  });\n\n  return function updateHamtDirectory(_x, _x2, _x3, _x4) {\n    return _ref.apply(this, arguments);\n  };\n}();\n/**\n * @param {PBLink[]} links\n * @param {Bucket<any>} rootBucket\n * @param {Bucket<any>} parentBucket\n * @param {number} positionAtParent\n */\n\n\nconst recreateHamtLevel = /*#__PURE__*/function () {\n  var _ref2 = _asyncToGenerator(function* (links, rootBucket, parentBucket, positionAtParent) {\n    // recreate this level of the HAMT\n    const bucket = new Bucket({\n      hash: rootBucket._options.hash,\n      bits: rootBucket._options.bits\n    }, parentBucket, positionAtParent);\n\n    parentBucket._putObjectAt(positionAtParent, bucket);\n\n    yield addLinksToHamtBucket(links, bucket, rootBucket);\n    return bucket;\n  });\n\n  return function recreateHamtLevel(_x5, _x6, _x7, _x8) {\n    return _ref2.apply(this, arguments);\n  };\n}();\n/**\n * @param {PBLink[]} links\n */\n\n\nconst recreateInitialHamtLevel = /*#__PURE__*/function () {\n  var _ref3 = _asyncToGenerator(function* (links) {\n    const bucket = createHAMT({\n      hashFn: hamtHashFn,\n      bits: hamtBucketBits\n    });\n    yield addLinksToHamtBucket(links, bucket, bucket);\n    return bucket;\n  });\n\n  return function recreateInitialHamtLevel(_x9) {\n    return _ref3.apply(this, arguments);\n  };\n}();\n/**\n * @param {PBLink[]} links\n * @param {Bucket<any>} bucket\n * @param {Bucket<any>} rootBucket\n */\n\n\nconst addLinksToHamtBucket = /*#__PURE__*/function () {\n  var _ref4 = _asyncToGenerator(function* (links, bucket, rootBucket) {\n    yield Promise.all(links.map(link => {\n      const linkName = link.Name || '';\n\n      if (linkName.length === 2) {\n        const pos = parseInt(linkName, 16);\n\n        bucket._putObjectAt(pos, new Bucket({\n          hash: rootBucket._options.hash,\n          bits: rootBucket._options.bits\n        }, bucket, pos));\n\n        return Promise.resolve();\n      }\n\n      return rootBucket.put(linkName.substring(2), {\n        size: link.Tsize,\n        cid: link.Hash\n      });\n    }));\n  });\n\n  return function addLinksToHamtBucket(_x10, _x11, _x12) {\n    return _ref4.apply(this, arguments);\n  };\n}();\n/**\n * @param {number} position\n */\n\n\nconst toPrefix = position => {\n  return position.toString(16).toUpperCase().padStart(2, '0').substring(0, 2);\n};\n/**\n * @param {MfsContext} context\n * @param {string} fileName\n * @param {PBNode} rootNode\n */\n\n\nconst generatePath = /*#__PURE__*/function () {\n  var _ref5 = _asyncToGenerator(function* (context, fileName, rootNode) {\n    // start at the root bucket and descend, loading nodes as we go\n    const rootBucket = yield recreateInitialHamtLevel(rootNode.Links);\n    const position = yield rootBucket._findNewBucketAndPos(fileName); // the path to the root bucket\n\n    /** @type {{ bucket: Bucket<any>, prefix: string, node?: PBNode }[]} */\n\n    const path = [{\n      bucket: position.bucket,\n      prefix: toPrefix(position.pos)\n    }];\n    let currentBucket = position.bucket;\n\n    while (currentBucket !== rootBucket) {\n      path.push({\n        bucket: currentBucket,\n        prefix: toPrefix(currentBucket._posAtParent)\n      }); // @ts-ignore - only the root bucket's parent will be undefined\n\n      currentBucket = currentBucket._parent;\n    }\n\n    path.reverse();\n    path[0].node = rootNode; // load PbNode for each path segment\n\n    for (let i = 0; i < path.length; i++) {\n      const segment = path[i];\n\n      if (!segment.node) {\n        throw new Error('Could not generate HAMT path');\n      } // find prefix in links\n\n\n      const link = segment.node.Links.filter(link => (link.Name || '').substring(0, 2) === segment.prefix).pop(); // entry was not in shard\n\n      if (!link) {\n        // reached bottom of tree, file will be added to the current bucket\n        log(`Link ${segment.prefix}${fileName} will be added`); // return path\n\n        continue;\n      } // found entry\n\n\n      if (link.Name === `${segment.prefix}${fileName}`) {\n        log(`Link ${segment.prefix}${fileName} will be replaced`); // file already existed, file will be added to the current bucket\n        // return path\n\n        continue;\n      } // found subshard\n\n\n      log(`Found subshard ${segment.prefix}`);\n      const block = yield context.repo.blocks.get(link.Hash);\n      const node = dagPb.decode(block); // subshard hasn't been loaded, descend to the next level of the HAMT\n\n      if (!path[i + 1]) {\n        log(`Loaded new subshard ${segment.prefix}`);\n        yield recreateHamtLevel(node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16));\n        const position = yield rootBucket._findNewBucketAndPos(fileName); // i--\n\n        path.push({\n          bucket: position.bucket,\n          prefix: toPrefix(position.pos),\n          node: node\n        });\n        continue;\n      }\n\n      const nextSegment = path[i + 1]; // add intermediate links to bucket\n\n      yield addLinksToHamtBucket(node.Links, nextSegment.bucket, rootBucket);\n      nextSegment.node = node;\n    }\n\n    yield rootBucket.put(fileName, true);\n    path.reverse();\n    return {\n      rootBucket,\n      path\n    };\n  });\n\n  return function generatePath(_x13, _x14, _x15) {\n    return _ref5.apply(this, arguments);\n  };\n}();\n/**\n * @param {MfsContext} context\n * @param {{ name: string, size: number, cid: CID }[]} contents\n * @param {object} [options]\n * @param {Mtime} [options.mtime]\n * @param {number} [options.mode]\n */\n\n\nconst createShard = /*#__PURE__*/function () {\n  var _ref6 = _asyncToGenerator(function* (context, contents, options = {}) {\n    const shard = new DirSharded({\n      root: true,\n      dir: true,\n      parent: undefined,\n      parentKey: undefined,\n      path: '',\n      dirty: true,\n      flat: false,\n      mtime: options.mtime,\n      mode: options.mode\n    }, options);\n\n    for (let i = 0; i < contents.length; i++) {\n      yield shard._bucket.put(contents[i].name, {\n        size: contents[i].size,\n        cid: contents[i].cid\n      });\n    }\n\n    const res = yield last(shard.flush(context.repo.blocks));\n\n    if (!res) {\n      throw new Error('Flushing shard yielded no result');\n    }\n\n    return res;\n  });\n\n  return function createShard(_x16, _x17) {\n    return _ref6.apply(this, arguments);\n  };\n}();\n\nmodule.exports = {\n  generatePath,\n  updateHamtDirectory,\n  recreateHamtLevel,\n  recreateInitialHamtLevel,\n  addLinksToHamtBucket,\n  toPrefix,\n  createShard\n};","map":null,"metadata":{},"sourceType":"script"}