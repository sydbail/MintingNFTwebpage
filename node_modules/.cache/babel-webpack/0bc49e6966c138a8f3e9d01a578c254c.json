{"ast":null,"code":"'use strict';\n\nvar _asyncToGenerator = require(\"/Users/sydneybailey/Internship/Demo/node_modules/@babel/runtime/helpers/asyncToGenerator\").default;\n\nconst dagPb = require('@ipld/dag-pb');\n\nconst {\n  CID\n} = require('multiformats/cid');\n\nconst log = require('debug')('ipfs:mfs:core:utils:add-link');\n\nconst {\n  UnixFS\n} = require('ipfs-unixfs');\n\nconst DirSharded = require('./dir-sharded');\n\nconst {\n  updateHamtDirectory,\n  recreateHamtLevel,\n  recreateInitialHamtLevel,\n  createShard,\n  toPrefix,\n  addLinksToHamtBucket\n} = require('./hamt-utils');\n\nconst errCode = require('err-code');\n\nconst last = require('it-last');\n/**\n * @typedef {import('ipfs-unixfs').Mtime} Mtime\n * @typedef {import('multiformats/cid').CIDVersion} CIDVersion\n * @typedef {import('hamt-sharding').Bucket<any>} Bucket\n * @typedef {import('../').MfsContext} MfsContext\n * @typedef {import('@ipld/dag-pb').PBNode} PBNode\n * @typedef {import('@ipld/dag-pb').PBLink} PBLink\n */\n\n/**\n * @param {MfsContext} context\n * @param {object} options\n * @param {CID} options.cid\n * @param {string} options.name\n * @param {number} options.size\n * @param {number} options.shardSplitThreshold\n * @param {string} options.hashAlg\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n * @param {CID} [options.parentCid]\n * @param {PBNode} [options.parent]\n */\n\n\nconst addLink = /*#__PURE__*/function () {\n  var _ref = _asyncToGenerator(function* (context, options) {\n    let parent = options.parent;\n\n    if (options.parentCid) {\n      const parentCid = CID.asCID(options.parentCid);\n\n      if (parentCid === null) {\n        throw errCode(new Error('Invalid CID passed to addLink'), 'EINVALIDPARENTCID');\n      }\n\n      if (parentCid.code !== dagPb.code) {\n        throw errCode(new Error('Unsupported codec. Only DAG-PB is supported'), 'EINVALIDPARENTCID');\n      }\n\n      log(`Loading parent node ${parentCid}`);\n      const block = yield context.repo.blocks.get(parentCid);\n      parent = dagPb.decode(block);\n    }\n\n    if (!parent) {\n      throw errCode(new Error('No parent node or CID passed to addLink'), 'EINVALIDPARENT');\n    }\n\n    if (!options.cid) {\n      throw errCode(new Error('No child cid passed to addLink'), 'EINVALIDCHILDCID');\n    }\n\n    if (!options.name) {\n      throw errCode(new Error('No child name passed to addLink'), 'EINVALIDCHILDNAME');\n    }\n\n    if (!options.size && options.size !== 0) {\n      throw errCode(new Error('No child size passed to addLink'), 'EINVALIDCHILDSIZE');\n    }\n\n    if (!parent.Data) {\n      throw errCode(new Error('Parent node with no data passed to addLink'), 'ERR_INVALID_PARENT');\n    }\n\n    const meta = UnixFS.unmarshal(parent.Data);\n\n    if (meta.type === 'hamt-sharded-directory') {\n      log('Adding link to sharded directory');\n      return addToShardedDirectory(context, { ...options,\n        parent\n      });\n    }\n\n    if (parent.Links.length >= options.shardSplitThreshold) {\n      log('Converting directory to sharded directory');\n      return convertToShardedDirectory(context, { ...options,\n        parent,\n        mtime: meta.mtime,\n        mode: meta.mode\n      });\n    }\n\n    log(`Adding ${options.name} (${options.cid}) to regular directory`);\n    return addToDirectory(context, { ...options,\n      parent\n    });\n  });\n\n  return function addLink(_x, _x2) {\n    return _ref.apply(this, arguments);\n  };\n}();\n/**\n * @param {MfsContext} context\n * @param {object} options\n * @param {CID} options.cid\n * @param {string} options.name\n * @param {number} options.size\n * @param {PBNode} options.parent\n * @param {string} options.hashAlg\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n * @param {Mtime} [options.mtime]\n * @param {number} [options.mode]\n */\n\n\nconst convertToShardedDirectory = /*#__PURE__*/function () {\n  var _ref2 = _asyncToGenerator(function* (context, options) {\n    const result = yield createShard(context, options.parent.Links.map(link => ({\n      name: link.Name || '',\n      size: link.Tsize || 0,\n      cid: link.Hash\n    })).concat({\n      name: options.name,\n      size: options.size,\n      cid: options.cid\n    }), options);\n    log(`Converted directory to sharded directory ${result.cid}`);\n    return result;\n  });\n\n  return function convertToShardedDirectory(_x3, _x4) {\n    return _ref2.apply(this, arguments);\n  };\n}();\n/**\n * @param {MfsContext} context\n * @param {object} options\n * @param {CID} options.cid\n * @param {string} options.name\n * @param {number} options.size\n * @param {PBNode} options.parent\n * @param {string} options.hashAlg\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n * @param {Mtime} [options.mtime]\n * @param {number} [options.mode]\n */\n\n\nconst addToDirectory = /*#__PURE__*/function () {\n  var _ref3 = _asyncToGenerator(function* (context, options) {\n    // Remove existing link if it exists\n    const parentLinks = options.parent.Links.filter(link => {\n      return link.Name !== options.name;\n    });\n    parentLinks.push({\n      Name: options.name,\n      Tsize: options.size,\n      Hash: options.cid\n    });\n\n    if (!options.parent.Data) {\n      throw errCode(new Error('Parent node with no data passed to addToDirectory'), 'ERR_INVALID_PARENT');\n    }\n\n    const node = UnixFS.unmarshal(options.parent.Data);\n    let data;\n\n    if (node.mtime) {\n      // Update mtime if previously set\n      const ms = Date.now();\n      const secs = Math.floor(ms / 1000);\n      node.mtime = {\n        secs: secs,\n        nsecs: (ms - secs * 1000) * 1000\n      };\n      data = node.marshal();\n    } else {\n      data = options.parent.Data;\n    }\n\n    options.parent = dagPb.prepare({\n      Data: data,\n      Links: parentLinks\n    }); // Persist the new parent PbNode\n\n    const hasher = yield context.hashers.getHasher(options.hashAlg);\n    const buf = dagPb.encode(options.parent);\n    const hash = yield hasher.digest(buf);\n    const cid = CID.create(options.cidVersion, dagPb.code, hash);\n\n    if (options.flush) {\n      yield context.repo.blocks.put(cid, buf);\n    }\n\n    return {\n      node: options.parent,\n      cid,\n      size: buf.length\n    };\n  });\n\n  return function addToDirectory(_x5, _x6) {\n    return _ref3.apply(this, arguments);\n  };\n}();\n/**\n * @param {MfsContext} context\n * @param {object} options\n * @param {CID} options.cid\n * @param {string} options.name\n * @param {number} options.size\n * @param {PBNode} options.parent\n * @param {string} options.hashAlg\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n */\n\n\nconst addToShardedDirectory = /*#__PURE__*/function () {\n  var _ref4 = _asyncToGenerator(function* (context, options) {\n    const {\n      shard,\n      path\n    } = yield addFileToShardedDirectory(context, options);\n    const result = yield last(shard.flush(context.repo.blocks));\n\n    if (!result) {\n      throw new Error('No result from flushing shard');\n    }\n\n    const block = yield context.repo.blocks.get(result.cid);\n    const node = dagPb.decode(block); // we have written out the shard, but only one sub-shard will have been written so replace it in the original shard\n\n    const parentLinks = options.parent.Links.filter(link => {\n      // TODO vmx 2021-03-31: Check that there cannot be multiple ones matching\n      // Remove the old link\n      return (link.Name || '').substring(0, 2) !== path[0].prefix;\n    });\n    const newLink = node.Links.find(link => (link.Name || '').substring(0, 2) === path[0].prefix);\n\n    if (!newLink) {\n      throw new Error(`No link found with prefix ${path[0].prefix}`);\n    }\n\n    parentLinks.push(newLink);\n    return updateHamtDirectory(context, parentLinks, path[0].bucket, options);\n  });\n\n  return function addToShardedDirectory(_x7, _x8) {\n    return _ref4.apply(this, arguments);\n  };\n}();\n/**\n * @param {MfsContext} context\n * @param {object} options\n * @param {CID} options.cid\n * @param {string} options.name\n * @param {number} options.size\n * @param {PBNode} options.parent\n * @param {string} options.hashAlg\n * @param {CIDVersion} options.cidVersion\n */\n\n\nconst addFileToShardedDirectory = /*#__PURE__*/function () {\n  var _ref5 = _asyncToGenerator(function* (context, options) {\n    const file = {\n      name: options.name,\n      cid: options.cid,\n      size: options.size\n    };\n\n    if (!options.parent.Data) {\n      throw errCode(new Error('Parent node with no data passed to addFileToShardedDirectory'), 'ERR_INVALID_PARENT');\n    } // start at the root bucket and descend, loading nodes as we go\n\n\n    const rootBucket = yield recreateInitialHamtLevel(options.parent.Links);\n    const node = UnixFS.unmarshal(options.parent.Data);\n    const shard = new DirSharded({\n      root: true,\n      dir: true,\n      parent: undefined,\n      parentKey: undefined,\n      path: '',\n      dirty: true,\n      flat: false,\n      mode: node.mode\n    }, options);\n    shard._bucket = rootBucket;\n\n    if (node.mtime) {\n      // update mtime if previously set\n      shard.mtime = {\n        secs: Math.round(Date.now() / 1000)\n      };\n    } // load subshards until the bucket & position no longer changes\n\n\n    const position = yield rootBucket._findNewBucketAndPos(file.name);\n    const path = toBucketPath(position);\n    path[0].node = options.parent;\n    let index = 0;\n\n    while (index < path.length) {\n      const segment = path[index];\n      index++;\n      const node = segment.node;\n\n      if (!node) {\n        throw new Error('Segment had no node');\n      }\n\n      const link = node.Links.find(link => (link.Name || '').substring(0, 2) === segment.prefix);\n\n      if (!link) {\n        // prefix is new, file will be added to the current bucket\n        log(`Link ${segment.prefix}${file.name} will be added`);\n        index = path.length;\n        break;\n      }\n\n      if (link.Name === `${segment.prefix}${file.name}`) {\n        // file already existed, file will be added to the current bucket\n        log(`Link ${segment.prefix}${file.name} will be replaced`);\n        index = path.length;\n        break;\n      }\n\n      if ((link.Name || '').length > 2) {\n        // another file had the same prefix, will be replaced with a subshard\n        log(`Link ${link.Name} ${link.Hash} will be replaced with a subshard`);\n        index = path.length;\n        break;\n      } // load sub-shard\n\n\n      log(`Found subshard ${segment.prefix}`);\n      const block = yield context.repo.blocks.get(link.Hash);\n      const subShard = dagPb.decode(block); // subshard hasn't been loaded, descend to the next level of the HAMT\n\n      if (!path[index]) {\n        log(`Loaded new subshard ${segment.prefix}`);\n        yield recreateHamtLevel(subShard.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16));\n        const position = yield rootBucket._findNewBucketAndPos(file.name);\n        path.push({\n          bucket: position.bucket,\n          prefix: toPrefix(position.pos),\n          node: subShard\n        });\n        break;\n      }\n\n      const nextSegment = path[index]; // add next levels worth of links to bucket\n\n      yield addLinksToHamtBucket(subShard.Links, nextSegment.bucket, rootBucket);\n      nextSegment.node = subShard;\n    } // finally add the new file into the shard\n\n\n    yield shard._bucket.put(file.name, {\n      size: file.size,\n      cid: file.cid\n    });\n    return {\n      shard,\n      path\n    };\n  });\n\n  return function addFileToShardedDirectory(_x9, _x10) {\n    return _ref5.apply(this, arguments);\n  };\n}();\n/**\n * @param {{ pos: number, bucket: Bucket }} position\n * @returns {{ bucket: Bucket, prefix: string, node?: PBNode }[]}\n */\n\n\nconst toBucketPath = position => {\n  const path = [{\n    bucket: position.bucket,\n    prefix: toPrefix(position.pos)\n  }];\n  let bucket = position.bucket._parent;\n  let positionInBucket = position.bucket._posAtParent;\n\n  while (bucket) {\n    path.push({\n      bucket,\n      prefix: toPrefix(positionInBucket)\n    });\n    positionInBucket = bucket._posAtParent;\n    bucket = bucket._parent;\n  }\n\n  path.reverse();\n  return path;\n};\n\nmodule.exports = addLink;","map":{"version":3,"sources":["/Users/sydneybailey/Internship/Demo/node_modules/ipfs-core/src/components/files/utils/add-link.js"],"names":["dagPb","require","CID","log","UnixFS","DirSharded","updateHamtDirectory","recreateHamtLevel","recreateInitialHamtLevel","createShard","toPrefix","addLinksToHamtBucket","errCode","last","addLink","context","options","parent","parentCid","asCID","Error","code","block","repo","blocks","get","decode","cid","name","size","Data","meta","unmarshal","type","addToShardedDirectory","Links","length","shardSplitThreshold","convertToShardedDirectory","mtime","mode","addToDirectory","result","map","link","Name","Tsize","Hash","concat","parentLinks","filter","push","node","data","ms","Date","now","secs","Math","floor","nsecs","marshal","prepare","hasher","hashers","getHasher","hashAlg","buf","encode","hash","digest","create","cidVersion","flush","put","shard","path","addFileToShardedDirectory","substring","prefix","newLink","find","bucket","file","rootBucket","root","dir","undefined","parentKey","dirty","flat","_bucket","round","position","_findNewBucketAndPos","toBucketPath","index","segment","subShard","parseInt","pos","nextSegment","_parent","positionInBucket","_posAtParent","reverse","module","exports"],"mappings":"AAAA;;;;AAEA,MAAMA,KAAK,GAAGC,OAAO,CAAC,cAAD,CAArB;;AACA,MAAM;AAAEC,EAAAA;AAAF,IAAUD,OAAO,CAAC,kBAAD,CAAvB;;AACA,MAAME,GAAG,GAAGF,OAAO,CAAC,OAAD,CAAP,CAAiB,8BAAjB,CAAZ;;AACA,MAAM;AAAEG,EAAAA;AAAF,IAAaH,OAAO,CAAC,aAAD,CAA1B;;AACA,MAAMI,UAAU,GAAGJ,OAAO,CAAC,eAAD,CAA1B;;AACA,MAAM;AACJK,EAAAA,mBADI;AAEJC,EAAAA,iBAFI;AAGJC,EAAAA,wBAHI;AAIJC,EAAAA,WAJI;AAKJC,EAAAA,QALI;AAMJC,EAAAA;AANI,IAOFV,OAAO,CAAC,cAAD,CAPX;;AAQA,MAAMW,OAAO,GAAGX,OAAO,CAAC,UAAD,CAAvB;;AACA,MAAMY,IAAI,GAAGZ,OAAO,CAAC,SAAD,CAApB;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACA,MAAMa,OAAO;AAAA,+BAAG,WAAOC,OAAP,EAAgBC,OAAhB,EAA4B;AAC1C,QAAIC,MAAM,GAAGD,OAAO,CAACC,MAArB;;AAEA,QAAID,OAAO,CAACE,SAAZ,EAAuB;AACrB,YAAMA,SAAS,GAAGhB,GAAG,CAACiB,KAAJ,CAAUH,OAAO,CAACE,SAAlB,CAAlB;;AACA,UAAIA,SAAS,KAAK,IAAlB,EAAwB;AACtB,cAAMN,OAAO,CAAC,IAAIQ,KAAJ,CAAU,+BAAV,CAAD,EAA6C,mBAA7C,CAAb;AACD;;AAED,UAAIF,SAAS,CAACG,IAAV,KAAmBrB,KAAK,CAACqB,IAA7B,EAAmC;AACjC,cAAMT,OAAO,CAAC,IAAIQ,KAAJ,CAAU,6CAAV,CAAD,EAA2D,mBAA3D,CAAb;AACD;;AAEDjB,MAAAA,GAAG,CAAE,uBAAsBe,SAAU,EAAlC,CAAH;AACA,YAAMI,KAAK,SAASP,OAAO,CAACQ,IAAR,CAAaC,MAAb,CAAoBC,GAApB,CAAwBP,SAAxB,CAApB;AACAD,MAAAA,MAAM,GAAGjB,KAAK,CAAC0B,MAAN,CAAaJ,KAAb,CAAT;AACD;;AAED,QAAI,CAACL,MAAL,EAAa;AACX,YAAML,OAAO,CAAC,IAAIQ,KAAJ,CAAU,yCAAV,CAAD,EAAuD,gBAAvD,CAAb;AACD;;AAED,QAAI,CAACJ,OAAO,CAACW,GAAb,EAAkB;AAChB,YAAMf,OAAO,CAAC,IAAIQ,KAAJ,CAAU,gCAAV,CAAD,EAA8C,kBAA9C,CAAb;AACD;;AAED,QAAI,CAACJ,OAAO,CAACY,IAAb,EAAmB;AACjB,YAAMhB,OAAO,CAAC,IAAIQ,KAAJ,CAAU,iCAAV,CAAD,EAA+C,mBAA/C,CAAb;AACD;;AAED,QAAI,CAACJ,OAAO,CAACa,IAAT,IAAiBb,OAAO,CAACa,IAAR,KAAiB,CAAtC,EAAyC;AACvC,YAAMjB,OAAO,CAAC,IAAIQ,KAAJ,CAAU,iCAAV,CAAD,EAA+C,mBAA/C,CAAb;AACD;;AAED,QAAI,CAACH,MAAM,CAACa,IAAZ,EAAkB;AAChB,YAAMlB,OAAO,CAAC,IAAIQ,KAAJ,CAAU,4CAAV,CAAD,EAA0D,oBAA1D,CAAb;AACD;;AAED,UAAMW,IAAI,GAAG3B,MAAM,CAAC4B,SAAP,CAAiBf,MAAM,CAACa,IAAxB,CAAb;;AAEA,QAAIC,IAAI,CAACE,IAAL,KAAc,wBAAlB,EAA4C;AAC1C9B,MAAAA,GAAG,CAAC,kCAAD,CAAH;AAEA,aAAO+B,qBAAqB,CAACnB,OAAD,EAAU,EACpC,GAAGC,OADiC;AAEpCC,QAAAA;AAFoC,OAAV,CAA5B;AAID;;AAED,QAAIA,MAAM,CAACkB,KAAP,CAAaC,MAAb,IAAuBpB,OAAO,CAACqB,mBAAnC,EAAwD;AACtDlC,MAAAA,GAAG,CAAC,2CAAD,CAAH;AAEA,aAAOmC,yBAAyB,CAACvB,OAAD,EAAU,EACxC,GAAGC,OADqC;AAExCC,QAAAA,MAFwC;AAGxCsB,QAAAA,KAAK,EAAER,IAAI,CAACQ,KAH4B;AAIxCC,QAAAA,IAAI,EAAET,IAAI,CAACS;AAJ6B,OAAV,CAAhC;AAMD;;AAEDrC,IAAAA,GAAG,CAAE,UAASa,OAAO,CAACY,IAAK,KAAIZ,OAAO,CAACW,GAAI,wBAAxC,CAAH;AAEA,WAAOc,cAAc,CAAC1B,OAAD,EAAU,EAC7B,GAAGC,OAD0B;AAE7BC,MAAAA;AAF6B,KAAV,CAArB;AAID,GAlEY;;AAAA,kBAAPH,OAAO;AAAA;AAAA;AAAA,GAAb;AAoEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACA,MAAMwB,yBAAyB;AAAA,gCAAG,WAAOvB,OAAP,EAAgBC,OAAhB,EAA4B;AAC5D,UAAM0B,MAAM,SAASjC,WAAW,CAACM,OAAD,EAAUC,OAAO,CAACC,MAAR,CAAekB,KAAf,CAAqBQ,GAArB,CAAyBC,IAAI,KAAK;AAC1EhB,MAAAA,IAAI,EAAGgB,IAAI,CAACC,IAAL,IAAa,EADsD;AAE1EhB,MAAAA,IAAI,EAAEe,IAAI,CAACE,KAAL,IAAc,CAFsD;AAG1EnB,MAAAA,GAAG,EAAEiB,IAAI,CAACG;AAHgE,KAAL,CAA7B,EAItCC,MAJsC,CAI/B;AACTpB,MAAAA,IAAI,EAAEZ,OAAO,CAACY,IADL;AAETC,MAAAA,IAAI,EAAEb,OAAO,CAACa,IAFL;AAGTF,MAAAA,GAAG,EAAEX,OAAO,CAACW;AAHJ,KAJ+B,CAAV,EAQ5BX,OAR4B,CAAhC;AAUAb,IAAAA,GAAG,CAAE,4CAA2CuC,MAAM,CAACf,GAAI,EAAxD,CAAH;AAEA,WAAOe,MAAP;AACD,GAd8B;;AAAA,kBAAzBJ,yBAAyB;AAAA;AAAA;AAAA,GAA/B;AAgBA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACA,MAAMG,cAAc;AAAA,gCAAG,WAAO1B,OAAP,EAAgBC,OAAhB,EAA4B;AACjD;AACA,UAAMiC,WAAW,GAAGjC,OAAO,CAACC,MAAR,CAAekB,KAAf,CAAqBe,MAArB,CAA6BN,IAAD,IAAU;AACxD,aAAOA,IAAI,CAACC,IAAL,KAAc7B,OAAO,CAACY,IAA7B;AACD,KAFmB,CAApB;AAGAqB,IAAAA,WAAW,CAACE,IAAZ,CAAiB;AACfN,MAAAA,IAAI,EAAE7B,OAAO,CAACY,IADC;AAEfkB,MAAAA,KAAK,EAAE9B,OAAO,CAACa,IAFA;AAGfkB,MAAAA,IAAI,EAAE/B,OAAO,CAACW;AAHC,KAAjB;;AAMA,QAAI,CAACX,OAAO,CAACC,MAAR,CAAea,IAApB,EAA0B;AACxB,YAAMlB,OAAO,CAAC,IAAIQ,KAAJ,CAAU,mDAAV,CAAD,EAAiE,oBAAjE,CAAb;AACD;;AAED,UAAMgC,IAAI,GAAGhD,MAAM,CAAC4B,SAAP,CAAiBhB,OAAO,CAACC,MAAR,CAAea,IAAhC,CAAb;AAEA,QAAIuB,IAAJ;;AACA,QAAID,IAAI,CAACb,KAAT,EAAgB;AACd;AACA,YAAMe,EAAE,GAAGC,IAAI,CAACC,GAAL,EAAX;AACA,YAAMC,IAAI,GAAGC,IAAI,CAACC,KAAL,CAAWL,EAAE,GAAG,IAAhB,CAAb;AAEAF,MAAAA,IAAI,CAACb,KAAL,GAAa;AACXkB,QAAAA,IAAI,EAAEA,IADK;AAEXG,QAAAA,KAAK,EAAE,CAACN,EAAE,GAAIG,IAAI,GAAG,IAAd,IAAuB;AAFnB,OAAb;AAKAJ,MAAAA,IAAI,GAAGD,IAAI,CAACS,OAAL,EAAP;AACD,KAXD,MAWO;AACLR,MAAAA,IAAI,GAAGrC,OAAO,CAACC,MAAR,CAAea,IAAtB;AACD;;AACDd,IAAAA,OAAO,CAACC,MAAR,GAAiBjB,KAAK,CAAC8D,OAAN,CAAc;AAC7BhC,MAAAA,IAAI,EAAEuB,IADuB;AAE7BlB,MAAAA,KAAK,EAAEc;AAFsB,KAAd,CAAjB,CAhCiD,CAqCjD;;AACA,UAAMc,MAAM,SAAShD,OAAO,CAACiD,OAAR,CAAgBC,SAAhB,CAA0BjD,OAAO,CAACkD,OAAlC,CAArB;AACA,UAAMC,GAAG,GAAGnE,KAAK,CAACoE,MAAN,CAAapD,OAAO,CAACC,MAArB,CAAZ;AACA,UAAMoD,IAAI,SAASN,MAAM,CAACO,MAAP,CAAcH,GAAd,CAAnB;AACA,UAAMxC,GAAG,GAAGzB,GAAG,CAACqE,MAAJ,CAAWvD,OAAO,CAACwD,UAAnB,EAA+BxE,KAAK,CAACqB,IAArC,EAA2CgD,IAA3C,CAAZ;;AAEA,QAAIrD,OAAO,CAACyD,KAAZ,EAAmB;AACjB,YAAM1D,OAAO,CAACQ,IAAR,CAAaC,MAAb,CAAoBkD,GAApB,CAAwB/C,GAAxB,EAA6BwC,GAA7B,CAAN;AACD;;AAED,WAAO;AACLf,MAAAA,IAAI,EAAEpC,OAAO,CAACC,MADT;AAELU,MAAAA,GAFK;AAGLE,MAAAA,IAAI,EAAEsC,GAAG,CAAC/B;AAHL,KAAP;AAKD,GApDmB;;AAAA,kBAAdK,cAAc;AAAA;AAAA;AAAA,GAApB;AAsDA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACA,MAAMP,qBAAqB;AAAA,gCAAG,WAAOnB,OAAP,EAAgBC,OAAhB,EAA4B;AACxD,UAAM;AACJ2D,MAAAA,KADI;AACGC,MAAAA;AADH,cAEIC,yBAAyB,CAAC9D,OAAD,EAAUC,OAAV,CAFnC;AAGA,UAAM0B,MAAM,SAAS7B,IAAI,CAAC8D,KAAK,CAACF,KAAN,CAAY1D,OAAO,CAACQ,IAAR,CAAaC,MAAzB,CAAD,CAAzB;;AAEA,QAAI,CAACkB,MAAL,EAAa;AACX,YAAM,IAAItB,KAAJ,CAAU,+BAAV,CAAN;AACD;;AAED,UAAME,KAAK,SAASP,OAAO,CAACQ,IAAR,CAAaC,MAAb,CAAoBC,GAApB,CAAwBiB,MAAM,CAACf,GAA/B,CAApB;AACA,UAAMyB,IAAI,GAAGpD,KAAK,CAAC0B,MAAN,CAAaJ,KAAb,CAAb,CAXwD,CAaxD;;AACA,UAAM2B,WAAW,GAAGjC,OAAO,CAACC,MAAR,CAAekB,KAAf,CAAqBe,MAArB,CAA6BN,IAAD,IAAU;AACxD;AACA;AACA,aAAO,CAACA,IAAI,CAACC,IAAL,IAAa,EAAd,EAAkBiC,SAAlB,CAA4B,CAA5B,EAA+B,CAA/B,MAAsCF,IAAI,CAAC,CAAD,CAAJ,CAAQG,MAArD;AACD,KAJmB,CAApB;AAMA,UAAMC,OAAO,GAAG5B,IAAI,CAACjB,KAAL,CACb8C,IADa,CACRrC,IAAI,IAAI,CAACA,IAAI,CAACC,IAAL,IAAa,EAAd,EAAkBiC,SAAlB,CAA4B,CAA5B,EAA+B,CAA/B,MAAsCF,IAAI,CAAC,CAAD,CAAJ,CAAQG,MAD9C,CAAhB;;AAGA,QAAI,CAACC,OAAL,EAAc;AACZ,YAAM,IAAI5D,KAAJ,CAAW,6BAA4BwD,IAAI,CAAC,CAAD,CAAJ,CAAQG,MAAO,EAAtD,CAAN;AACD;;AAED9B,IAAAA,WAAW,CAACE,IAAZ,CAAiB6B,OAAjB;AAEA,WAAO1E,mBAAmB,CAACS,OAAD,EAAUkC,WAAV,EAAuB2B,IAAI,CAAC,CAAD,CAAJ,CAAQM,MAA/B,EAAuClE,OAAvC,CAA1B;AACD,GA9B0B;;AAAA,kBAArBkB,qBAAqB;AAAA;AAAA;AAAA,GAA3B;AAgCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACA,MAAM2C,yBAAyB;AAAA,gCAAG,WAAO9D,OAAP,EAAgBC,OAAhB,EAA4B;AAC5D,UAAMmE,IAAI,GAAG;AACXvD,MAAAA,IAAI,EAAEZ,OAAO,CAACY,IADH;AAEXD,MAAAA,GAAG,EAAEX,OAAO,CAACW,GAFF;AAGXE,MAAAA,IAAI,EAAEb,OAAO,CAACa;AAHH,KAAb;;AAMA,QAAI,CAACb,OAAO,CAACC,MAAR,CAAea,IAApB,EAA0B;AACxB,YAAMlB,OAAO,CAAC,IAAIQ,KAAJ,CAAU,8DAAV,CAAD,EAA4E,oBAA5E,CAAb;AACD,KAT2D,CAW5D;;;AACA,UAAMgE,UAAU,SAAS5E,wBAAwB,CAACQ,OAAO,CAACC,MAAR,CAAekB,KAAhB,CAAjD;AACA,UAAMiB,IAAI,GAAGhD,MAAM,CAAC4B,SAAP,CAAiBhB,OAAO,CAACC,MAAR,CAAea,IAAhC,CAAb;AAEA,UAAM6C,KAAK,GAAG,IAAItE,UAAJ,CAAe;AAC3BgF,MAAAA,IAAI,EAAE,IADqB;AAE3BC,MAAAA,GAAG,EAAE,IAFsB;AAG3BrE,MAAAA,MAAM,EAAEsE,SAHmB;AAI3BC,MAAAA,SAAS,EAAED,SAJgB;AAK3BX,MAAAA,IAAI,EAAE,EALqB;AAM3Ba,MAAAA,KAAK,EAAE,IANoB;AAO3BC,MAAAA,IAAI,EAAE,KAPqB;AAQ3BlD,MAAAA,IAAI,EAAEY,IAAI,CAACZ;AARgB,KAAf,EASXxB,OATW,CAAd;AAUA2D,IAAAA,KAAK,CAACgB,OAAN,GAAgBP,UAAhB;;AAEA,QAAIhC,IAAI,CAACb,KAAT,EAAgB;AACd;AACAoC,MAAAA,KAAK,CAACpC,KAAN,GAAc;AACZkB,QAAAA,IAAI,EAAEC,IAAI,CAACkC,KAAL,CAAWrC,IAAI,CAACC,GAAL,KAAa,IAAxB;AADM,OAAd;AAGD,KAhC2D,CAkC5D;;;AACA,UAAMqC,QAAQ,SAAST,UAAU,CAACU,oBAAX,CAAgCX,IAAI,CAACvD,IAArC,CAAvB;AACA,UAAMgD,IAAI,GAAGmB,YAAY,CAACF,QAAD,CAAzB;AACAjB,IAAAA,IAAI,CAAC,CAAD,CAAJ,CAAQxB,IAAR,GAAepC,OAAO,CAACC,MAAvB;AACA,QAAI+E,KAAK,GAAG,CAAZ;;AAEA,WAAOA,KAAK,GAAGpB,IAAI,CAACxC,MAApB,EAA4B;AAC1B,YAAM6D,OAAO,GAAGrB,IAAI,CAACoB,KAAD,CAApB;AACAA,MAAAA,KAAK;AACL,YAAM5C,IAAI,GAAG6C,OAAO,CAAC7C,IAArB;;AAEA,UAAI,CAACA,IAAL,EAAW;AACT,cAAM,IAAIhC,KAAJ,CAAU,qBAAV,CAAN;AACD;;AAED,YAAMwB,IAAI,GAAGQ,IAAI,CAACjB,KAAL,CACV8C,IADU,CACLrC,IAAI,IAAI,CAACA,IAAI,CAACC,IAAL,IAAa,EAAd,EAAkBiC,SAAlB,CAA4B,CAA5B,EAA+B,CAA/B,MAAsCmB,OAAO,CAAClB,MADjD,CAAb;;AAGA,UAAI,CAACnC,IAAL,EAAW;AACT;AACAzC,QAAAA,GAAG,CAAE,QAAO8F,OAAO,CAAClB,MAAO,GAAEI,IAAI,CAACvD,IAAK,gBAApC,CAAH;AACAoE,QAAAA,KAAK,GAAGpB,IAAI,CAACxC,MAAb;AAEA;AACD;;AAED,UAAIQ,IAAI,CAACC,IAAL,KAAe,GAAEoD,OAAO,CAAClB,MAAO,GAAEI,IAAI,CAACvD,IAAK,EAAhD,EAAmD;AACjD;AACAzB,QAAAA,GAAG,CAAE,QAAO8F,OAAO,CAAClB,MAAO,GAAEI,IAAI,CAACvD,IAAK,mBAApC,CAAH;AACAoE,QAAAA,KAAK,GAAGpB,IAAI,CAACxC,MAAb;AAEA;AACD;;AAED,UAAI,CAACQ,IAAI,CAACC,IAAL,IAAa,EAAd,EAAkBT,MAAlB,GAA2B,CAA/B,EAAkC;AAChC;AACAjC,QAAAA,GAAG,CAAE,QAAOyC,IAAI,CAACC,IAAK,IAAGD,IAAI,CAACG,IAAK,mCAAhC,CAAH;AACAiD,QAAAA,KAAK,GAAGpB,IAAI,CAACxC,MAAb;AAEA;AACD,OAlCyB,CAoC1B;;;AACAjC,MAAAA,GAAG,CAAE,kBAAiB8F,OAAO,CAAClB,MAAO,EAAlC,CAAH;AACA,YAAMzD,KAAK,SAASP,OAAO,CAACQ,IAAR,CAAaC,MAAb,CAAoBC,GAApB,CAAwBmB,IAAI,CAACG,IAA7B,CAApB;AACA,YAAMmD,QAAQ,GAAGlG,KAAK,CAAC0B,MAAN,CAAaJ,KAAb,CAAjB,CAvC0B,CAyC1B;;AACA,UAAI,CAACsD,IAAI,CAACoB,KAAD,CAAT,EAAkB;AAChB7F,QAAAA,GAAG,CAAE,uBAAsB8F,OAAO,CAAClB,MAAO,EAAvC,CAAH;AACA,cAAMxE,iBAAiB,CAAC2F,QAAQ,CAAC/D,KAAV,EAAiBiD,UAAjB,EAA6Ba,OAAO,CAACf,MAArC,EAA6CiB,QAAQ,CAACF,OAAO,CAAClB,MAAT,EAAiB,EAAjB,CAArD,CAAvB;AAEA,cAAMc,QAAQ,SAAST,UAAU,CAACU,oBAAX,CAAgCX,IAAI,CAACvD,IAArC,CAAvB;AAEAgD,QAAAA,IAAI,CAACzB,IAAL,CAAU;AACR+B,UAAAA,MAAM,EAAEW,QAAQ,CAACX,MADT;AAERH,UAAAA,MAAM,EAAErE,QAAQ,CAACmF,QAAQ,CAACO,GAAV,CAFR;AAGRhD,UAAAA,IAAI,EAAE8C;AAHE,SAAV;AAMA;AACD;;AAED,YAAMG,WAAW,GAAGzB,IAAI,CAACoB,KAAD,CAAxB,CAzD0B,CA2D1B;;AACA,YAAMrF,oBAAoB,CAACuF,QAAQ,CAAC/D,KAAV,EAAiBkE,WAAW,CAACnB,MAA7B,EAAqCE,UAArC,CAA1B;AAEAiB,MAAAA,WAAW,CAACjD,IAAZ,GAAmB8C,QAAnB;AACD,KAvG2D,CAyG5D;;;AACA,UAAMvB,KAAK,CAACgB,OAAN,CAAcjB,GAAd,CAAkBS,IAAI,CAACvD,IAAvB,EAA6B;AACjCC,MAAAA,IAAI,EAAEsD,IAAI,CAACtD,IADsB;AAEjCF,MAAAA,GAAG,EAAEwD,IAAI,CAACxD;AAFuB,KAA7B,CAAN;AAKA,WAAO;AACLgD,MAAAA,KADK;AACEC,MAAAA;AADF,KAAP;AAGD,GAlH8B;;AAAA,kBAAzBC,yBAAyB;AAAA;AAAA;AAAA,GAA/B;AAoHA;AACA;AACA;AACA;;;AACA,MAAMkB,YAAY,GAAIF,QAAD,IAAc;AACjC,QAAMjB,IAAI,GAAG,CAAC;AACZM,IAAAA,MAAM,EAAEW,QAAQ,CAACX,MADL;AAEZH,IAAAA,MAAM,EAAErE,QAAQ,CAACmF,QAAQ,CAACO,GAAV;AAFJ,GAAD,CAAb;AAKA,MAAIlB,MAAM,GAAGW,QAAQ,CAACX,MAAT,CAAgBoB,OAA7B;AACA,MAAIC,gBAAgB,GAAGV,QAAQ,CAACX,MAAT,CAAgBsB,YAAvC;;AAEA,SAAOtB,MAAP,EAAe;AACbN,IAAAA,IAAI,CAACzB,IAAL,CAAU;AACR+B,MAAAA,MADQ;AAERH,MAAAA,MAAM,EAAErE,QAAQ,CAAC6F,gBAAD;AAFR,KAAV;AAKAA,IAAAA,gBAAgB,GAAGrB,MAAM,CAACsB,YAA1B;AACAtB,IAAAA,MAAM,GAAGA,MAAM,CAACoB,OAAhB;AACD;;AAED1B,EAAAA,IAAI,CAAC6B,OAAL;AAEA,SAAO7B,IAAP;AACD,CAtBD;;AAwBA8B,MAAM,CAACC,OAAP,GAAiB7F,OAAjB","sourcesContent":["'use strict'\n\nconst dagPb = require('@ipld/dag-pb')\nconst { CID } = require('multiformats/cid')\nconst log = require('debug')('ipfs:mfs:core:utils:add-link')\nconst { UnixFS } = require('ipfs-unixfs')\nconst DirSharded = require('./dir-sharded')\nconst {\n  updateHamtDirectory,\n  recreateHamtLevel,\n  recreateInitialHamtLevel,\n  createShard,\n  toPrefix,\n  addLinksToHamtBucket\n} = require('./hamt-utils')\nconst errCode = require('err-code')\nconst last = require('it-last')\n\n/**\n * @typedef {import('ipfs-unixfs').Mtime} Mtime\n * @typedef {import('multiformats/cid').CIDVersion} CIDVersion\n * @typedef {import('hamt-sharding').Bucket<any>} Bucket\n * @typedef {import('../').MfsContext} MfsContext\n * @typedef {import('@ipld/dag-pb').PBNode} PBNode\n * @typedef {import('@ipld/dag-pb').PBLink} PBLink\n */\n\n/**\n * @param {MfsContext} context\n * @param {object} options\n * @param {CID} options.cid\n * @param {string} options.name\n * @param {number} options.size\n * @param {number} options.shardSplitThreshold\n * @param {string} options.hashAlg\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n * @param {CID} [options.parentCid]\n * @param {PBNode} [options.parent]\n */\nconst addLink = async (context, options) => {\n  let parent = options.parent\n\n  if (options.parentCid) {\n    const parentCid = CID.asCID(options.parentCid)\n    if (parentCid === null) {\n      throw errCode(new Error('Invalid CID passed to addLink'), 'EINVALIDPARENTCID')\n    }\n\n    if (parentCid.code !== dagPb.code) {\n      throw errCode(new Error('Unsupported codec. Only DAG-PB is supported'), 'EINVALIDPARENTCID')\n    }\n\n    log(`Loading parent node ${parentCid}`)\n    const block = await context.repo.blocks.get(parentCid)\n    parent = dagPb.decode(block)\n  }\n\n  if (!parent) {\n    throw errCode(new Error('No parent node or CID passed to addLink'), 'EINVALIDPARENT')\n  }\n\n  if (!options.cid) {\n    throw errCode(new Error('No child cid passed to addLink'), 'EINVALIDCHILDCID')\n  }\n\n  if (!options.name) {\n    throw errCode(new Error('No child name passed to addLink'), 'EINVALIDCHILDNAME')\n  }\n\n  if (!options.size && options.size !== 0) {\n    throw errCode(new Error('No child size passed to addLink'), 'EINVALIDCHILDSIZE')\n  }\n\n  if (!parent.Data) {\n    throw errCode(new Error('Parent node with no data passed to addLink'), 'ERR_INVALID_PARENT')\n  }\n\n  const meta = UnixFS.unmarshal(parent.Data)\n\n  if (meta.type === 'hamt-sharded-directory') {\n    log('Adding link to sharded directory')\n\n    return addToShardedDirectory(context, {\n      ...options,\n      parent\n    })\n  }\n\n  if (parent.Links.length >= options.shardSplitThreshold) {\n    log('Converting directory to sharded directory')\n\n    return convertToShardedDirectory(context, {\n      ...options,\n      parent,\n      mtime: meta.mtime,\n      mode: meta.mode\n    })\n  }\n\n  log(`Adding ${options.name} (${options.cid}) to regular directory`)\n\n  return addToDirectory(context, {\n    ...options,\n    parent\n  })\n}\n\n/**\n * @param {MfsContext} context\n * @param {object} options\n * @param {CID} options.cid\n * @param {string} options.name\n * @param {number} options.size\n * @param {PBNode} options.parent\n * @param {string} options.hashAlg\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n * @param {Mtime} [options.mtime]\n * @param {number} [options.mode]\n */\nconst convertToShardedDirectory = async (context, options) => {\n  const result = await createShard(context, options.parent.Links.map(link => ({\n    name: (link.Name || ''),\n    size: link.Tsize || 0,\n    cid: link.Hash\n  })).concat({\n    name: options.name,\n    size: options.size,\n    cid: options.cid\n  }), options)\n\n  log(`Converted directory to sharded directory ${result.cid}`)\n\n  return result\n}\n\n/**\n * @param {MfsContext} context\n * @param {object} options\n * @param {CID} options.cid\n * @param {string} options.name\n * @param {number} options.size\n * @param {PBNode} options.parent\n * @param {string} options.hashAlg\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n * @param {Mtime} [options.mtime]\n * @param {number} [options.mode]\n */\nconst addToDirectory = async (context, options) => {\n  // Remove existing link if it exists\n  const parentLinks = options.parent.Links.filter((link) => {\n    return link.Name !== options.name\n  })\n  parentLinks.push({\n    Name: options.name,\n    Tsize: options.size,\n    Hash: options.cid\n  })\n\n  if (!options.parent.Data) {\n    throw errCode(new Error('Parent node with no data passed to addToDirectory'), 'ERR_INVALID_PARENT')\n  }\n\n  const node = UnixFS.unmarshal(options.parent.Data)\n\n  let data\n  if (node.mtime) {\n    // Update mtime if previously set\n    const ms = Date.now()\n    const secs = Math.floor(ms / 1000)\n\n    node.mtime = {\n      secs: secs,\n      nsecs: (ms - (secs * 1000)) * 1000\n    }\n\n    data = node.marshal()\n  } else {\n    data = options.parent.Data\n  }\n  options.parent = dagPb.prepare({\n    Data: data,\n    Links: parentLinks\n  })\n\n  // Persist the new parent PbNode\n  const hasher = await context.hashers.getHasher(options.hashAlg)\n  const buf = dagPb.encode(options.parent)\n  const hash = await hasher.digest(buf)\n  const cid = CID.create(options.cidVersion, dagPb.code, hash)\n\n  if (options.flush) {\n    await context.repo.blocks.put(cid, buf)\n  }\n\n  return {\n    node: options.parent,\n    cid,\n    size: buf.length\n  }\n}\n\n/**\n * @param {MfsContext} context\n * @param {object} options\n * @param {CID} options.cid\n * @param {string} options.name\n * @param {number} options.size\n * @param {PBNode} options.parent\n * @param {string} options.hashAlg\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n */\nconst addToShardedDirectory = async (context, options) => {\n  const {\n    shard, path\n  } = await addFileToShardedDirectory(context, options)\n  const result = await last(shard.flush(context.repo.blocks))\n\n  if (!result) {\n    throw new Error('No result from flushing shard')\n  }\n\n  const block = await context.repo.blocks.get(result.cid)\n  const node = dagPb.decode(block)\n\n  // we have written out the shard, but only one sub-shard will have been written so replace it in the original shard\n  const parentLinks = options.parent.Links.filter((link) => {\n    // TODO vmx 2021-03-31: Check that there cannot be multiple ones matching\n    // Remove the old link\n    return (link.Name || '').substring(0, 2) !== path[0].prefix\n  })\n\n  const newLink = node.Links\n    .find(link => (link.Name || '').substring(0, 2) === path[0].prefix)\n\n  if (!newLink) {\n    throw new Error(`No link found with prefix ${path[0].prefix}`)\n  }\n\n  parentLinks.push(newLink)\n\n  return updateHamtDirectory(context, parentLinks, path[0].bucket, options)\n}\n\n/**\n * @param {MfsContext} context\n * @param {object} options\n * @param {CID} options.cid\n * @param {string} options.name\n * @param {number} options.size\n * @param {PBNode} options.parent\n * @param {string} options.hashAlg\n * @param {CIDVersion} options.cidVersion\n */\nconst addFileToShardedDirectory = async (context, options) => {\n  const file = {\n    name: options.name,\n    cid: options.cid,\n    size: options.size\n  }\n\n  if (!options.parent.Data) {\n    throw errCode(new Error('Parent node with no data passed to addFileToShardedDirectory'), 'ERR_INVALID_PARENT')\n  }\n\n  // start at the root bucket and descend, loading nodes as we go\n  const rootBucket = await recreateInitialHamtLevel(options.parent.Links)\n  const node = UnixFS.unmarshal(options.parent.Data)\n\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: undefined,\n    parentKey: undefined,\n    path: '',\n    dirty: true,\n    flat: false,\n    mode: node.mode\n  }, options)\n  shard._bucket = rootBucket\n\n  if (node.mtime) {\n    // update mtime if previously set\n    shard.mtime = {\n      secs: Math.round(Date.now() / 1000)\n    }\n  }\n\n  // load subshards until the bucket & position no longer changes\n  const position = await rootBucket._findNewBucketAndPos(file.name)\n  const path = toBucketPath(position)\n  path[0].node = options.parent\n  let index = 0\n\n  while (index < path.length) {\n    const segment = path[index]\n    index++\n    const node = segment.node\n\n    if (!node) {\n      throw new Error('Segment had no node')\n    }\n\n    const link = node.Links\n      .find(link => (link.Name || '').substring(0, 2) === segment.prefix)\n\n    if (!link) {\n      // prefix is new, file will be added to the current bucket\n      log(`Link ${segment.prefix}${file.name} will be added`)\n      index = path.length\n\n      break\n    }\n\n    if (link.Name === `${segment.prefix}${file.name}`) {\n      // file already existed, file will be added to the current bucket\n      log(`Link ${segment.prefix}${file.name} will be replaced`)\n      index = path.length\n\n      break\n    }\n\n    if ((link.Name || '').length > 2) {\n      // another file had the same prefix, will be replaced with a subshard\n      log(`Link ${link.Name} ${link.Hash} will be replaced with a subshard`)\n      index = path.length\n\n      break\n    }\n\n    // load sub-shard\n    log(`Found subshard ${segment.prefix}`)\n    const block = await context.repo.blocks.get(link.Hash)\n    const subShard = dagPb.decode(block)\n\n    // subshard hasn't been loaded, descend to the next level of the HAMT\n    if (!path[index]) {\n      log(`Loaded new subshard ${segment.prefix}`)\n      await recreateHamtLevel(subShard.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16))\n\n      const position = await rootBucket._findNewBucketAndPos(file.name)\n\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: subShard\n      })\n\n      break\n    }\n\n    const nextSegment = path[index]\n\n    // add next levels worth of links to bucket\n    await addLinksToHamtBucket(subShard.Links, nextSegment.bucket, rootBucket)\n\n    nextSegment.node = subShard\n  }\n\n  // finally add the new file into the shard\n  await shard._bucket.put(file.name, {\n    size: file.size,\n    cid: file.cid\n  })\n\n  return {\n    shard, path\n  }\n}\n\n/**\n * @param {{ pos: number, bucket: Bucket }} position\n * @returns {{ bucket: Bucket, prefix: string, node?: PBNode }[]}\n */\nconst toBucketPath = (position) => {\n  const path = [{\n    bucket: position.bucket,\n    prefix: toPrefix(position.pos)\n  }]\n\n  let bucket = position.bucket._parent\n  let positionInBucket = position.bucket._posAtParent\n\n  while (bucket) {\n    path.push({\n      bucket,\n      prefix: toPrefix(positionInBucket)\n    })\n\n    positionInBucket = bucket._posAtParent\n    bucket = bucket._parent\n  }\n\n  path.reverse()\n\n  return path\n}\n\nmodule.exports = addLink\n"]},"metadata":{},"sourceType":"script"}