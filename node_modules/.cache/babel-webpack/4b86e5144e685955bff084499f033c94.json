{"ast":null,"code":"import { decodeErrPrefix } from './common.js';\nimport { Type } from './token.js';\nimport { jump, quick } from './jump.js';\nconst defaultDecodeOptions = {\n  strict: false,\n  allowIndefinite: true,\n  allowUndefined: true,\n  allowBigInt: true\n};\n\nclass Tokeniser {\n  constructor(data, options = {}) {\n    this.pos = 0;\n    this.data = data;\n    this.options = options;\n  }\n\n  done() {\n    return this.pos >= this.data.length;\n  }\n\n  next() {\n    const byt = this.data[this.pos];\n    let token = quick[byt];\n\n    if (token === undefined) {\n      const decoder = jump[byt];\n\n      if (!decoder) {\n        throw new Error(`${decodeErrPrefix} no decoder for major type ${byt >>> 5} (byte 0x${byt.toString(16).padStart(2, '0')})`);\n      }\n\n      const minor = byt & 31;\n      token = decoder(this.data, this.pos, minor, this.options);\n    }\n\n    this.pos += token.encodedLength;\n    return token;\n  }\n\n}\n\nconst DONE = Symbol.for('DONE');\nconst BREAK = Symbol.for('BREAK');\n\nfunction tokenToArray(token, tokeniser, options) {\n  const arr = [];\n\n  for (let i = 0; i < token.value; i++) {\n    const value = tokensToObject(tokeniser, options);\n\n    if (value === BREAK) {\n      if (token.value === Infinity) {\n        break;\n      }\n\n      throw new Error(`${decodeErrPrefix} got unexpected break to lengthed array`);\n    }\n\n    if (value === DONE) {\n      throw new Error(`${decodeErrPrefix} found array but not enough entries (got ${i}, expected ${token.value})`);\n    }\n\n    arr[i] = value;\n  }\n\n  return arr;\n}\n\nfunction tokenToMap(token, tokeniser, options) {\n  const useMaps = options.useMaps === true;\n  const obj = useMaps ? undefined : {};\n  const m = useMaps ? new Map() : undefined;\n\n  for (let i = 0; i < token.value; i++) {\n    const key = tokensToObject(tokeniser, options);\n\n    if (key === BREAK) {\n      if (token.value === Infinity) {\n        break;\n      }\n\n      throw new Error(`${decodeErrPrefix} got unexpected break to lengthed map`);\n    }\n\n    if (key === DONE) {\n      throw new Error(`${decodeErrPrefix} found map but not enough entries (got ${i} [no key], expected ${token.value})`);\n    }\n\n    if (useMaps !== true && typeof key !== 'string') {\n      throw new Error(`${decodeErrPrefix} non-string keys not supported (got ${typeof key})`);\n    }\n\n    const value = tokensToObject(tokeniser, options);\n\n    if (value === DONE) {\n      throw new Error(`${decodeErrPrefix} found map but not enough entries (got ${i} [no value], expected ${token.value})`);\n    }\n\n    if (useMaps) {\n      m.set(key, value);\n    } else {\n      obj[key] = value;\n    }\n  }\n\n  return useMaps ? m : obj;\n}\n\nfunction tokensToObject(tokeniser, options) {\n  if (tokeniser.done()) {\n    return DONE;\n  }\n\n  const token = tokeniser.next();\n\n  if (token.type === Type.break) {\n    return BREAK;\n  }\n\n  if (token.type.terminal) {\n    return token.value;\n  }\n\n  if (token.type === Type.array) {\n    return tokenToArray(token, tokeniser, options);\n  }\n\n  if (token.type === Type.map) {\n    return tokenToMap(token, tokeniser, options);\n  }\n\n  if (token.type === Type.tag) {\n    if (options.tags && typeof options.tags[token.value] === 'function') {\n      const tagged = tokensToObject(tokeniser, options);\n      return options.tags[token.value](tagged);\n    }\n\n    throw new Error(`${decodeErrPrefix} tag not supported (${token.value})`);\n  }\n\n  throw new Error('unsupported');\n}\n\nfunction decode(data, options) {\n  if (!(data instanceof Uint8Array)) {\n    throw new Error(`${decodeErrPrefix} data to decode must be a Uint8Array`);\n  }\n\n  options = Object.assign({}, defaultDecodeOptions, options);\n  const tokeniser = options.tokenizer || new Tokeniser(data, options);\n  const decoded = tokensToObject(tokeniser, options);\n\n  if (decoded === DONE) {\n    throw new Error(`${decodeErrPrefix} did not find any content to decode`);\n  }\n\n  if (decoded === BREAK) {\n    throw new Error(`${decodeErrPrefix} got unexpected break`);\n  }\n\n  if (!tokeniser.done()) {\n    throw new Error(`${decodeErrPrefix} too many terminals, data makes no sense`);\n  }\n\n  return decoded;\n}\n\nexport { Tokeniser, tokensToObject, decode };","map":null,"metadata":{},"sourceType":"module"}