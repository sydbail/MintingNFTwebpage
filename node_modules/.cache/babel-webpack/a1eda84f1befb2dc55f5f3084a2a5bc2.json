{"ast":null,"code":"'use strict';\n\nvar _asyncToGenerator = require(\"/Users/sydneybailey/Internship/contract-testing/node_modules/@babel/runtime/helpers/asyncToGenerator\").default;\n\nconst dagPb = require('@ipld/dag-pb');\n\nconst {\n  CID\n} = require('multiformats/cid');\n\nconst log = require('debug')('ipfs:mfs:core:utils:add-link');\n\nconst {\n  UnixFS\n} = require('ipfs-unixfs');\n\nconst DirSharded = require('./dir-sharded');\n\nconst {\n  updateHamtDirectory,\n  recreateHamtLevel,\n  recreateInitialHamtLevel,\n  createShard,\n  toPrefix,\n  addLinksToHamtBucket\n} = require('./hamt-utils');\n\nconst errCode = require('err-code');\n\nconst last = require('it-last');\n/**\n * @typedef {import('ipfs-unixfs').Mtime} Mtime\n * @typedef {import('multiformats/cid').CIDVersion} CIDVersion\n * @typedef {import('hamt-sharding').Bucket<any>} Bucket\n * @typedef {import('../').MfsContext} MfsContext\n * @typedef {import('@ipld/dag-pb').PBNode} PBNode\n * @typedef {import('@ipld/dag-pb').PBLink} PBLink\n */\n\n/**\n * @param {MfsContext} context\n * @param {object} options\n * @param {CID} options.cid\n * @param {string} options.name\n * @param {number} options.size\n * @param {number} options.shardSplitThreshold\n * @param {string} options.hashAlg\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n * @param {CID} [options.parentCid]\n * @param {PBNode} [options.parent]\n */\n\n\nconst addLink = /*#__PURE__*/function () {\n  var _ref = _asyncToGenerator(function* (context, options) {\n    let parent = options.parent;\n\n    if (options.parentCid) {\n      const parentCid = CID.asCID(options.parentCid);\n\n      if (parentCid === null) {\n        throw errCode(new Error('Invalid CID passed to addLink'), 'EINVALIDPARENTCID');\n      }\n\n      if (parentCid.code !== dagPb.code) {\n        throw errCode(new Error('Unsupported codec. Only DAG-PB is supported'), 'EINVALIDPARENTCID');\n      }\n\n      log(`Loading parent node ${parentCid}`);\n      const block = yield context.repo.blocks.get(parentCid);\n      parent = dagPb.decode(block);\n    }\n\n    if (!parent) {\n      throw errCode(new Error('No parent node or CID passed to addLink'), 'EINVALIDPARENT');\n    }\n\n    if (!options.cid) {\n      throw errCode(new Error('No child cid passed to addLink'), 'EINVALIDCHILDCID');\n    }\n\n    if (!options.name) {\n      throw errCode(new Error('No child name passed to addLink'), 'EINVALIDCHILDNAME');\n    }\n\n    if (!options.size && options.size !== 0) {\n      throw errCode(new Error('No child size passed to addLink'), 'EINVALIDCHILDSIZE');\n    }\n\n    if (!parent.Data) {\n      throw errCode(new Error('Parent node with no data passed to addLink'), 'ERR_INVALID_PARENT');\n    }\n\n    const meta = UnixFS.unmarshal(parent.Data);\n\n    if (meta.type === 'hamt-sharded-directory') {\n      log('Adding link to sharded directory');\n      return addToShardedDirectory(context, { ...options,\n        parent\n      });\n    }\n\n    if (parent.Links.length >= options.shardSplitThreshold) {\n      log('Converting directory to sharded directory');\n      return convertToShardedDirectory(context, { ...options,\n        parent,\n        mtime: meta.mtime,\n        mode: meta.mode\n      });\n    }\n\n    log(`Adding ${options.name} (${options.cid}) to regular directory`);\n    return addToDirectory(context, { ...options,\n      parent\n    });\n  });\n\n  return function addLink(_x, _x2) {\n    return _ref.apply(this, arguments);\n  };\n}();\n/**\n * @param {MfsContext} context\n * @param {object} options\n * @param {CID} options.cid\n * @param {string} options.name\n * @param {number} options.size\n * @param {PBNode} options.parent\n * @param {string} options.hashAlg\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n * @param {Mtime} [options.mtime]\n * @param {number} [options.mode]\n */\n\n\nconst convertToShardedDirectory = /*#__PURE__*/function () {\n  var _ref2 = _asyncToGenerator(function* (context, options) {\n    const result = yield createShard(context, options.parent.Links.map(link => ({\n      name: link.Name || '',\n      size: link.Tsize || 0,\n      cid: link.Hash\n    })).concat({\n      name: options.name,\n      size: options.size,\n      cid: options.cid\n    }), options);\n    log(`Converted directory to sharded directory ${result.cid}`);\n    return result;\n  });\n\n  return function convertToShardedDirectory(_x3, _x4) {\n    return _ref2.apply(this, arguments);\n  };\n}();\n/**\n * @param {MfsContext} context\n * @param {object} options\n * @param {CID} options.cid\n * @param {string} options.name\n * @param {number} options.size\n * @param {PBNode} options.parent\n * @param {string} options.hashAlg\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n * @param {Mtime} [options.mtime]\n * @param {number} [options.mode]\n */\n\n\nconst addToDirectory = /*#__PURE__*/function () {\n  var _ref3 = _asyncToGenerator(function* (context, options) {\n    // Remove existing link if it exists\n    const parentLinks = options.parent.Links.filter(link => {\n      return link.Name !== options.name;\n    });\n    parentLinks.push({\n      Name: options.name,\n      Tsize: options.size,\n      Hash: options.cid\n    });\n\n    if (!options.parent.Data) {\n      throw errCode(new Error('Parent node with no data passed to addToDirectory'), 'ERR_INVALID_PARENT');\n    }\n\n    const node = UnixFS.unmarshal(options.parent.Data);\n    let data;\n\n    if (node.mtime) {\n      // Update mtime if previously set\n      const ms = Date.now();\n      const secs = Math.floor(ms / 1000);\n      node.mtime = {\n        secs: secs,\n        nsecs: (ms - secs * 1000) * 1000\n      };\n      data = node.marshal();\n    } else {\n      data = options.parent.Data;\n    }\n\n    options.parent = dagPb.prepare({\n      Data: data,\n      Links: parentLinks\n    }); // Persist the new parent PbNode\n\n    const hasher = yield context.hashers.getHasher(options.hashAlg);\n    const buf = dagPb.encode(options.parent);\n    const hash = yield hasher.digest(buf);\n    const cid = CID.create(options.cidVersion, dagPb.code, hash);\n\n    if (options.flush) {\n      yield context.repo.blocks.put(cid, buf);\n    }\n\n    return {\n      node: options.parent,\n      cid,\n      size: buf.length\n    };\n  });\n\n  return function addToDirectory(_x5, _x6) {\n    return _ref3.apply(this, arguments);\n  };\n}();\n/**\n * @param {MfsContext} context\n * @param {object} options\n * @param {CID} options.cid\n * @param {string} options.name\n * @param {number} options.size\n * @param {PBNode} options.parent\n * @param {string} options.hashAlg\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n */\n\n\nconst addToShardedDirectory = /*#__PURE__*/function () {\n  var _ref4 = _asyncToGenerator(function* (context, options) {\n    const {\n      shard,\n      path\n    } = yield addFileToShardedDirectory(context, options);\n    const result = yield last(shard.flush(context.repo.blocks));\n\n    if (!result) {\n      throw new Error('No result from flushing shard');\n    }\n\n    const block = yield context.repo.blocks.get(result.cid);\n    const node = dagPb.decode(block); // we have written out the shard, but only one sub-shard will have been written so replace it in the original shard\n\n    const parentLinks = options.parent.Links.filter(link => {\n      // TODO vmx 2021-03-31: Check that there cannot be multiple ones matching\n      // Remove the old link\n      return (link.Name || '').substring(0, 2) !== path[0].prefix;\n    });\n    const newLink = node.Links.find(link => (link.Name || '').substring(0, 2) === path[0].prefix);\n\n    if (!newLink) {\n      throw new Error(`No link found with prefix ${path[0].prefix}`);\n    }\n\n    parentLinks.push(newLink);\n    return updateHamtDirectory(context, parentLinks, path[0].bucket, options);\n  });\n\n  return function addToShardedDirectory(_x7, _x8) {\n    return _ref4.apply(this, arguments);\n  };\n}();\n/**\n * @param {MfsContext} context\n * @param {object} options\n * @param {CID} options.cid\n * @param {string} options.name\n * @param {number} options.size\n * @param {PBNode} options.parent\n * @param {string} options.hashAlg\n * @param {CIDVersion} options.cidVersion\n */\n\n\nconst addFileToShardedDirectory = /*#__PURE__*/function () {\n  var _ref5 = _asyncToGenerator(function* (context, options) {\n    const file = {\n      name: options.name,\n      cid: options.cid,\n      size: options.size\n    };\n\n    if (!options.parent.Data) {\n      throw errCode(new Error('Parent node with no data passed to addFileToShardedDirectory'), 'ERR_INVALID_PARENT');\n    } // start at the root bucket and descend, loading nodes as we go\n\n\n    const rootBucket = yield recreateInitialHamtLevel(options.parent.Links);\n    const node = UnixFS.unmarshal(options.parent.Data);\n    const shard = new DirSharded({\n      root: true,\n      dir: true,\n      parent: undefined,\n      parentKey: undefined,\n      path: '',\n      dirty: true,\n      flat: false,\n      mode: node.mode\n    }, options);\n    shard._bucket = rootBucket;\n\n    if (node.mtime) {\n      // update mtime if previously set\n      shard.mtime = {\n        secs: Math.round(Date.now() / 1000)\n      };\n    } // load subshards until the bucket & position no longer changes\n\n\n    const position = yield rootBucket._findNewBucketAndPos(file.name);\n    const path = toBucketPath(position);\n    path[0].node = options.parent;\n    let index = 0;\n\n    while (index < path.length) {\n      const segment = path[index];\n      index++;\n      const node = segment.node;\n\n      if (!node) {\n        throw new Error('Segment had no node');\n      }\n\n      const link = node.Links.find(link => (link.Name || '').substring(0, 2) === segment.prefix);\n\n      if (!link) {\n        // prefix is new, file will be added to the current bucket\n        log(`Link ${segment.prefix}${file.name} will be added`);\n        index = path.length;\n        break;\n      }\n\n      if (link.Name === `${segment.prefix}${file.name}`) {\n        // file already existed, file will be added to the current bucket\n        log(`Link ${segment.prefix}${file.name} will be replaced`);\n        index = path.length;\n        break;\n      }\n\n      if ((link.Name || '').length > 2) {\n        // another file had the same prefix, will be replaced with a subshard\n        log(`Link ${link.Name} ${link.Hash} will be replaced with a subshard`);\n        index = path.length;\n        break;\n      } // load sub-shard\n\n\n      log(`Found subshard ${segment.prefix}`);\n      const block = yield context.repo.blocks.get(link.Hash);\n      const subShard = dagPb.decode(block); // subshard hasn't been loaded, descend to the next level of the HAMT\n\n      if (!path[index]) {\n        log(`Loaded new subshard ${segment.prefix}`);\n        yield recreateHamtLevel(subShard.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16));\n        const position = yield rootBucket._findNewBucketAndPos(file.name);\n        path.push({\n          bucket: position.bucket,\n          prefix: toPrefix(position.pos),\n          node: subShard\n        });\n        break;\n      }\n\n      const nextSegment = path[index]; // add next levels worth of links to bucket\n\n      yield addLinksToHamtBucket(subShard.Links, nextSegment.bucket, rootBucket);\n      nextSegment.node = subShard;\n    } // finally add the new file into the shard\n\n\n    yield shard._bucket.put(file.name, {\n      size: file.size,\n      cid: file.cid\n    });\n    return {\n      shard,\n      path\n    };\n  });\n\n  return function addFileToShardedDirectory(_x9, _x10) {\n    return _ref5.apply(this, arguments);\n  };\n}();\n/**\n * @param {{ pos: number, bucket: Bucket }} position\n * @returns {{ bucket: Bucket, prefix: string, node?: PBNode }[]}\n */\n\n\nconst toBucketPath = position => {\n  const path = [{\n    bucket: position.bucket,\n    prefix: toPrefix(position.pos)\n  }];\n  let bucket = position.bucket._parent;\n  let positionInBucket = position.bucket._posAtParent;\n\n  while (bucket) {\n    path.push({\n      bucket,\n      prefix: toPrefix(positionInBucket)\n    });\n    positionInBucket = bucket._posAtParent;\n    bucket = bucket._parent;\n  }\n\n  path.reverse();\n  return path;\n};\n\nmodule.exports = addLink;","map":null,"metadata":{},"sourceType":"script"}