{"ast":null,"code":"'use strict';\n\nvar _asyncToGenerator = require(\"/Users/sydneybailey/Internship/contract-testing/node_modules/@babel/runtime/helpers/asyncToGenerator\").default;\n\nconst dagPb = require('@ipld/dag-pb');\n\nconst {\n  Bucket,\n  createHAMT\n} = require('hamt-sharding');\n\nconst DirSharded = require('./dir-sharded');\n\nconst log = require('debug')('ipfs:mfs:core:utils:hamt-utils');\n\nconst {\n  UnixFS\n} = require('ipfs-unixfs');\n\nconst last = require('it-last');\n\nconst {\n  CID\n} = require('multiformats/cid');\n\nconst {\n  hamtHashCode,\n  hamtHashFn,\n  hamtBucketBits\n} = require('./hamt-constants');\n/**\n * @typedef {import('multiformats/cid').CIDVersion} CIDVersion\n * @typedef {import('ipfs-unixfs').Mtime} Mtime\n * @typedef {import('../').MfsContext} MfsContext\n * @typedef {import('@ipld/dag-pb').PBNode} PBNode\n * @typedef {import('@ipld/dag-pb').PBLink} PBLink\n */\n\n/**\n * @param {MfsContext} context\n * @param {PBLink[]} links\n * @param {Bucket<any>} bucket\n * @param {object} options\n * @param {PBNode} options.parent\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n * @param {string} options.hashAlg\n */\n\n\nconst updateHamtDirectory = /*#__PURE__*/function () {\n  var _ref = _asyncToGenerator(function* (context, links, bucket, options) {\n    if (!options.parent.Data) {\n      throw new Error('Could not update HAMT directory because parent had no data');\n    } // update parent with new bit field\n\n\n    const data = Uint8Array.from(bucket._children.bitField().reverse());\n    const node = UnixFS.unmarshal(options.parent.Data);\n    const dir = new UnixFS({\n      type: 'hamt-sharded-directory',\n      data,\n      fanout: bucket.tableSize(),\n      hashType: hamtHashCode,\n      mode: node.mode,\n      mtime: node.mtime\n    });\n    const hasher = yield context.hashers.getHasher(options.hashAlg);\n    const parent = {\n      Data: dir.marshal(),\n      Links: links.sort((a, b) => (a.Name || '').localeCompare(b.Name || ''))\n    };\n    const buf = dagPb.encode(parent);\n    const hash = yield hasher.digest(buf);\n    const cid = CID.create(options.cidVersion, dagPb.code, hash);\n\n    if (options.flush) {\n      yield context.repo.blocks.put(cid, buf);\n    }\n\n    return {\n      node: parent,\n      cid,\n      size: links.reduce((sum, link) => sum + (link.Tsize || 0), buf.length)\n    };\n  });\n\n  return function updateHamtDirectory(_x, _x2, _x3, _x4) {\n    return _ref.apply(this, arguments);\n  };\n}();\n/**\n * @param {PBLink[]} links\n * @param {Bucket<any>} rootBucket\n * @param {Bucket<any>} parentBucket\n * @param {number} positionAtParent\n */\n\n\nconst recreateHamtLevel = /*#__PURE__*/function () {\n  var _ref2 = _asyncToGenerator(function* (links, rootBucket, parentBucket, positionAtParent) {\n    // recreate this level of the HAMT\n    const bucket = new Bucket({\n      hash: rootBucket._options.hash,\n      bits: rootBucket._options.bits\n    }, parentBucket, positionAtParent);\n\n    parentBucket._putObjectAt(positionAtParent, bucket);\n\n    yield addLinksToHamtBucket(links, bucket, rootBucket);\n    return bucket;\n  });\n\n  return function recreateHamtLevel(_x5, _x6, _x7, _x8) {\n    return _ref2.apply(this, arguments);\n  };\n}();\n/**\n * @param {PBLink[]} links\n */\n\n\nconst recreateInitialHamtLevel = /*#__PURE__*/function () {\n  var _ref3 = _asyncToGenerator(function* (links) {\n    const bucket = createHAMT({\n      hashFn: hamtHashFn,\n      bits: hamtBucketBits\n    });\n    yield addLinksToHamtBucket(links, bucket, bucket);\n    return bucket;\n  });\n\n  return function recreateInitialHamtLevel(_x9) {\n    return _ref3.apply(this, arguments);\n  };\n}();\n/**\n * @param {PBLink[]} links\n * @param {Bucket<any>} bucket\n * @param {Bucket<any>} rootBucket\n */\n\n\nconst addLinksToHamtBucket = /*#__PURE__*/function () {\n  var _ref4 = _asyncToGenerator(function* (links, bucket, rootBucket) {\n    yield Promise.all(links.map(link => {\n      const linkName = link.Name || '';\n\n      if (linkName.length === 2) {\n        const pos = parseInt(linkName, 16);\n\n        bucket._putObjectAt(pos, new Bucket({\n          hash: rootBucket._options.hash,\n          bits: rootBucket._options.bits\n        }, bucket, pos));\n\n        return Promise.resolve();\n      }\n\n      return rootBucket.put(linkName.substring(2), {\n        size: link.Tsize,\n        cid: link.Hash\n      });\n    }));\n  });\n\n  return function addLinksToHamtBucket(_x10, _x11, _x12) {\n    return _ref4.apply(this, arguments);\n  };\n}();\n/**\n * @param {number} position\n */\n\n\nconst toPrefix = position => {\n  return position.toString(16).toUpperCase().padStart(2, '0').substring(0, 2);\n};\n/**\n * @param {MfsContext} context\n * @param {string} fileName\n * @param {PBNode} rootNode\n */\n\n\nconst generatePath = /*#__PURE__*/function () {\n  var _ref5 = _asyncToGenerator(function* (context, fileName, rootNode) {\n    // start at the root bucket and descend, loading nodes as we go\n    const rootBucket = yield recreateInitialHamtLevel(rootNode.Links);\n    const position = yield rootBucket._findNewBucketAndPos(fileName); // the path to the root bucket\n\n    /** @type {{ bucket: Bucket<any>, prefix: string, node?: PBNode }[]} */\n\n    const path = [{\n      bucket: position.bucket,\n      prefix: toPrefix(position.pos)\n    }];\n    let currentBucket = position.bucket;\n\n    while (currentBucket !== rootBucket) {\n      path.push({\n        bucket: currentBucket,\n        prefix: toPrefix(currentBucket._posAtParent)\n      }); // @ts-ignore - only the root bucket's parent will be undefined\n\n      currentBucket = currentBucket._parent;\n    }\n\n    path.reverse();\n    path[0].node = rootNode; // load PbNode for each path segment\n\n    for (let i = 0; i < path.length; i++) {\n      const segment = path[i];\n\n      if (!segment.node) {\n        throw new Error('Could not generate HAMT path');\n      } // find prefix in links\n\n\n      const link = segment.node.Links.filter(link => (link.Name || '').substring(0, 2) === segment.prefix).pop(); // entry was not in shard\n\n      if (!link) {\n        // reached bottom of tree, file will be added to the current bucket\n        log(`Link ${segment.prefix}${fileName} will be added`); // return path\n\n        continue;\n      } // found entry\n\n\n      if (link.Name === `${segment.prefix}${fileName}`) {\n        log(`Link ${segment.prefix}${fileName} will be replaced`); // file already existed, file will be added to the current bucket\n        // return path\n\n        continue;\n      } // found subshard\n\n\n      log(`Found subshard ${segment.prefix}`);\n      const block = yield context.repo.blocks.get(link.Hash);\n      const node = dagPb.decode(block); // subshard hasn't been loaded, descend to the next level of the HAMT\n\n      if (!path[i + 1]) {\n        log(`Loaded new subshard ${segment.prefix}`);\n        yield recreateHamtLevel(node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16));\n        const position = yield rootBucket._findNewBucketAndPos(fileName); // i--\n\n        path.push({\n          bucket: position.bucket,\n          prefix: toPrefix(position.pos),\n          node: node\n        });\n        continue;\n      }\n\n      const nextSegment = path[i + 1]; // add intermediate links to bucket\n\n      yield addLinksToHamtBucket(node.Links, nextSegment.bucket, rootBucket);\n      nextSegment.node = node;\n    }\n\n    yield rootBucket.put(fileName, true);\n    path.reverse();\n    return {\n      rootBucket,\n      path\n    };\n  });\n\n  return function generatePath(_x13, _x14, _x15) {\n    return _ref5.apply(this, arguments);\n  };\n}();\n/**\n * @param {MfsContext} context\n * @param {{ name: string, size: number, cid: CID }[]} contents\n * @param {object} [options]\n * @param {Mtime} [options.mtime]\n * @param {number} [options.mode]\n */\n\n\nconst createShard = /*#__PURE__*/function () {\n  var _ref6 = _asyncToGenerator(function* (context, contents, options = {}) {\n    const shard = new DirSharded({\n      root: true,\n      dir: true,\n      parent: undefined,\n      parentKey: undefined,\n      path: '',\n      dirty: true,\n      flat: false,\n      mtime: options.mtime,\n      mode: options.mode\n    }, options);\n\n    for (let i = 0; i < contents.length; i++) {\n      yield shard._bucket.put(contents[i].name, {\n        size: contents[i].size,\n        cid: contents[i].cid\n      });\n    }\n\n    const res = yield last(shard.flush(context.repo.blocks));\n\n    if (!res) {\n      throw new Error('Flushing shard yielded no result');\n    }\n\n    return res;\n  });\n\n  return function createShard(_x16, _x17) {\n    return _ref6.apply(this, arguments);\n  };\n}();\n\nmodule.exports = {\n  generatePath,\n  updateHamtDirectory,\n  recreateHamtLevel,\n  recreateInitialHamtLevel,\n  addLinksToHamtBucket,\n  toPrefix,\n  createShard\n};","map":{"version":3,"sources":["/Users/sydneybailey/Internship/contract-testing/node_modules/ipfs-core/src/components/files/utils/hamt-utils.js"],"names":["dagPb","require","Bucket","createHAMT","DirSharded","log","UnixFS","last","CID","hamtHashCode","hamtHashFn","hamtBucketBits","updateHamtDirectory","context","links","bucket","options","parent","Data","Error","data","Uint8Array","from","_children","bitField","reverse","node","unmarshal","dir","type","fanout","tableSize","hashType","mode","mtime","hasher","hashers","getHasher","hashAlg","marshal","Links","sort","a","b","Name","localeCompare","buf","encode","hash","digest","cid","create","cidVersion","code","flush","repo","blocks","put","size","reduce","sum","link","Tsize","length","recreateHamtLevel","rootBucket","parentBucket","positionAtParent","_options","bits","_putObjectAt","addLinksToHamtBucket","recreateInitialHamtLevel","hashFn","Promise","all","map","linkName","pos","parseInt","resolve","substring","Hash","toPrefix","position","toString","toUpperCase","padStart","generatePath","fileName","rootNode","_findNewBucketAndPos","path","prefix","currentBucket","push","_posAtParent","_parent","i","segment","filter","pop","block","get","decode","nextSegment","createShard","contents","shard","root","undefined","parentKey","dirty","flat","_bucket","name","res","module","exports"],"mappings":"AAAA;;;;AAEA,MAAMA,KAAK,GAAGC,OAAO,CAAC,cAAD,CAArB;;AACA,MAAM;AACJC,EAAAA,MADI;AAEJC,EAAAA;AAFI,IAGFF,OAAO,CAAC,eAAD,CAHX;;AAIA,MAAMG,UAAU,GAAGH,OAAO,CAAC,eAAD,CAA1B;;AACA,MAAMI,GAAG,GAAGJ,OAAO,CAAC,OAAD,CAAP,CAAiB,gCAAjB,CAAZ;;AACA,MAAM;AAAEK,EAAAA;AAAF,IAAaL,OAAO,CAAC,aAAD,CAA1B;;AACA,MAAMM,IAAI,GAAGN,OAAO,CAAC,SAAD,CAApB;;AACA,MAAM;AAAEO,EAAAA;AAAF,IAAUP,OAAO,CAAC,kBAAD,CAAvB;;AACA,MAAM;AACJQ,EAAAA,YADI;AAEJC,EAAAA,UAFI;AAGJC,EAAAA;AAHI,IAIFV,OAAO,CAAC,kBAAD,CAJX;AAMA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACA,MAAMW,mBAAmB;AAAA,+BAAG,WAAOC,OAAP,EAAgBC,KAAhB,EAAuBC,MAAvB,EAA+BC,OAA/B,EAA2C;AACrE,QAAI,CAACA,OAAO,CAACC,MAAR,CAAeC,IAApB,EAA0B;AACxB,YAAM,IAAIC,KAAJ,CAAU,4DAAV,CAAN;AACD,KAHoE,CAKrE;;;AACA,UAAMC,IAAI,GAAGC,UAAU,CAACC,IAAX,CAAgBP,MAAM,CAACQ,SAAP,CAAiBC,QAAjB,GAA4BC,OAA5B,EAAhB,CAAb;AACA,UAAMC,IAAI,GAAGpB,MAAM,CAACqB,SAAP,CAAiBX,OAAO,CAACC,MAAR,CAAeC,IAAhC,CAAb;AACA,UAAMU,GAAG,GAAG,IAAItB,MAAJ,CAAW;AACrBuB,MAAAA,IAAI,EAAE,wBADe;AAErBT,MAAAA,IAFqB;AAGrBU,MAAAA,MAAM,EAAEf,MAAM,CAACgB,SAAP,EAHa;AAIrBC,MAAAA,QAAQ,EAAEvB,YAJW;AAKrBwB,MAAAA,IAAI,EAAEP,IAAI,CAACO,IALU;AAMrBC,MAAAA,KAAK,EAAER,IAAI,CAACQ;AANS,KAAX,CAAZ;AASA,UAAMC,MAAM,SAAStB,OAAO,CAACuB,OAAR,CAAgBC,SAAhB,CAA0BrB,OAAO,CAACsB,OAAlC,CAArB;AACA,UAAMrB,MAAM,GAAG;AACbC,MAAAA,IAAI,EAAEU,GAAG,CAACW,OAAJ,EADO;AAEbC,MAAAA,KAAK,EAAE1B,KAAK,CAAC2B,IAAN,CAAW,CAACC,CAAD,EAAIC,CAAJ,KAAU,CAACD,CAAC,CAACE,IAAF,IAAU,EAAX,EAAeC,aAAf,CAA6BF,CAAC,CAACC,IAAF,IAAU,EAAvC,CAArB;AAFM,KAAf;AAIA,UAAME,GAAG,GAAG9C,KAAK,CAAC+C,MAAN,CAAa9B,MAAb,CAAZ;AACA,UAAM+B,IAAI,SAASb,MAAM,CAACc,MAAP,CAAcH,GAAd,CAAnB;AACA,UAAMI,GAAG,GAAG1C,GAAG,CAAC2C,MAAJ,CAAWnC,OAAO,CAACoC,UAAnB,EAA+BpD,KAAK,CAACqD,IAArC,EAA2CL,IAA3C,CAAZ;;AAEA,QAAIhC,OAAO,CAACsC,KAAZ,EAAmB;AACjB,YAAMzC,OAAO,CAAC0C,IAAR,CAAaC,MAAb,CAAoBC,GAApB,CAAwBP,GAAxB,EAA6BJ,GAA7B,CAAN;AACD;;AAED,WAAO;AACLpB,MAAAA,IAAI,EAAET,MADD;AAELiC,MAAAA,GAFK;AAGLQ,MAAAA,IAAI,EAAE5C,KAAK,CAAC6C,MAAN,CAAa,CAACC,GAAD,EAAMC,IAAN,KAAeD,GAAG,IAAIC,IAAI,CAACC,KAAL,IAAc,CAAlB,CAA/B,EAAqDhB,GAAG,CAACiB,MAAzD;AAHD,KAAP;AAKD,GAnCwB;;AAAA,kBAAnBnD,mBAAmB;AAAA;AAAA;AAAA,GAAzB;AAqCA;AACA;AACA;AACA;AACA;AACA;;;AACA,MAAMoD,iBAAiB;AAAA,gCAAG,WAAOlD,KAAP,EAAcmD,UAAd,EAA0BC,YAA1B,EAAwCC,gBAAxC,EAA6D;AACrF;AACA,UAAMpD,MAAM,GAAG,IAAIb,MAAJ,CAAW;AACxB8C,MAAAA,IAAI,EAAEiB,UAAU,CAACG,QAAX,CAAoBpB,IADF;AAExBqB,MAAAA,IAAI,EAAEJ,UAAU,CAACG,QAAX,CAAoBC;AAFF,KAAX,EAGZH,YAHY,EAGEC,gBAHF,CAAf;;AAIAD,IAAAA,YAAY,CAACI,YAAb,CAA0BH,gBAA1B,EAA4CpD,MAA5C;;AAEA,UAAMwD,oBAAoB,CAACzD,KAAD,EAAQC,MAAR,EAAgBkD,UAAhB,CAA1B;AAEA,WAAOlD,MAAP;AACD,GAXsB;;AAAA,kBAAjBiD,iBAAiB;AAAA;AAAA;AAAA,GAAvB;AAaA;AACA;AACA;;;AACA,MAAMQ,wBAAwB;AAAA,gCAAG,WAAO1D,KAAP,EAAiB;AAChD,UAAMC,MAAM,GAAGZ,UAAU,CAAC;AACxBsE,MAAAA,MAAM,EAAE/D,UADgB;AAExB2D,MAAAA,IAAI,EAAE1D;AAFkB,KAAD,CAAzB;AAKA,UAAM4D,oBAAoB,CAACzD,KAAD,EAAQC,MAAR,EAAgBA,MAAhB,CAA1B;AAEA,WAAOA,MAAP;AACD,GAT6B;;AAAA,kBAAxByD,wBAAwB;AAAA;AAAA;AAAA,GAA9B;AAWA;AACA;AACA;AACA;AACA;;;AACA,MAAMD,oBAAoB;AAAA,gCAAG,WAAOzD,KAAP,EAAcC,MAAd,EAAsBkD,UAAtB,EAAqC;AAChE,UAAMS,OAAO,CAACC,GAAR,CACJ7D,KAAK,CAAC8D,GAAN,CAAUf,IAAI,IAAI;AAChB,YAAMgB,QAAQ,GAAIhB,IAAI,CAACjB,IAAL,IAAa,EAA/B;;AAEA,UAAIiC,QAAQ,CAACd,MAAT,KAAoB,CAAxB,EAA2B;AACzB,cAAMe,GAAG,GAAGC,QAAQ,CAACF,QAAD,EAAW,EAAX,CAApB;;AAEA9D,QAAAA,MAAM,CAACuD,YAAP,CAAoBQ,GAApB,EAAyB,IAAI5E,MAAJ,CAAW;AAClC8C,UAAAA,IAAI,EAAEiB,UAAU,CAACG,QAAX,CAAoBpB,IADQ;AAElCqB,UAAAA,IAAI,EAAEJ,UAAU,CAACG,QAAX,CAAoBC;AAFQ,SAAX,EAGtBtD,MAHsB,EAGd+D,GAHc,CAAzB;;AAKA,eAAOJ,OAAO,CAACM,OAAR,EAAP;AACD;;AAED,aAAOf,UAAU,CAACR,GAAX,CAAeoB,QAAQ,CAACI,SAAT,CAAmB,CAAnB,CAAf,EAAsC;AAC3CvB,QAAAA,IAAI,EAAEG,IAAI,CAACC,KADgC;AAE3CZ,QAAAA,GAAG,EAAEW,IAAI,CAACqB;AAFiC,OAAtC,CAAP;AAID,KAlBD,CADI,CAAN;AAqBD,GAtByB;;AAAA,kBAApBX,oBAAoB;AAAA;AAAA;AAAA,GAA1B;AAwBA;AACA;AACA;;;AACA,MAAMY,QAAQ,GAAIC,QAAD,IAAc;AAC7B,SAAOA,QAAQ,CACZC,QADI,CACK,EADL,EAEJC,WAFI,GAGJC,QAHI,CAGK,CAHL,EAGQ,GAHR,EAIJN,SAJI,CAIM,CAJN,EAIS,CAJT,CAAP;AAKD,CAND;AAQA;AACA;AACA;AACA;AACA;;;AACA,MAAMO,YAAY;AAAA,gCAAG,WAAO3E,OAAP,EAAgB4E,QAAhB,EAA0BC,QAA1B,EAAuC;AAC1D;AACA,UAAMzB,UAAU,SAASO,wBAAwB,CAACkB,QAAQ,CAAClD,KAAV,CAAjD;AACA,UAAM4C,QAAQ,SAASnB,UAAU,CAAC0B,oBAAX,CAAgCF,QAAhC,CAAvB,CAH0D,CAK1D;;AACA;;AACA,UAAMG,IAAI,GAAG,CAAC;AACZ7E,MAAAA,MAAM,EAAEqE,QAAQ,CAACrE,MADL;AAEZ8E,MAAAA,MAAM,EAAEV,QAAQ,CAACC,QAAQ,CAACN,GAAV;AAFJ,KAAD,CAAb;AAIA,QAAIgB,aAAa,GAAGV,QAAQ,CAACrE,MAA7B;;AAEA,WAAO+E,aAAa,KAAK7B,UAAzB,EAAqC;AACnC2B,MAAAA,IAAI,CAACG,IAAL,CAAU;AACRhF,QAAAA,MAAM,EAAE+E,aADA;AAERD,QAAAA,MAAM,EAAEV,QAAQ,CAACW,aAAa,CAACE,YAAf;AAFR,OAAV,EADmC,CAMnC;;AACAF,MAAAA,aAAa,GAAGA,aAAa,CAACG,OAA9B;AACD;;AAEDL,IAAAA,IAAI,CAACnE,OAAL;AACAmE,IAAAA,IAAI,CAAC,CAAD,CAAJ,CAAQlE,IAAR,GAAegE,QAAf,CAxB0D,CA0B1D;;AACA,SAAK,IAAIQ,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGN,IAAI,CAAC7B,MAAzB,EAAiCmC,CAAC,EAAlC,EAAsC;AACpC,YAAMC,OAAO,GAAGP,IAAI,CAACM,CAAD,CAApB;;AAEA,UAAI,CAACC,OAAO,CAACzE,IAAb,EAAmB;AACjB,cAAM,IAAIP,KAAJ,CAAU,8BAAV,CAAN;AACD,OALmC,CAOpC;;;AACA,YAAM0C,IAAI,GAAGsC,OAAO,CAACzE,IAAR,CAAac,KAAb,CACV4D,MADU,CACHvC,IAAI,IAAI,CAACA,IAAI,CAACjB,IAAL,IAAa,EAAd,EAAkBqC,SAAlB,CAA4B,CAA5B,EAA+B,CAA/B,MAAsCkB,OAAO,CAACN,MADnD,EAEVQ,GAFU,EAAb,CARoC,CAYpC;;AACA,UAAI,CAACxC,IAAL,EAAW;AACT;AACAxD,QAAAA,GAAG,CAAE,QAAO8F,OAAO,CAACN,MAAO,GAAEJ,QAAS,gBAAnC,CAAH,CAFS,CAGT;;AACA;AACD,OAlBmC,CAoBpC;;;AACA,UAAI5B,IAAI,CAACjB,IAAL,KAAe,GAAEuD,OAAO,CAACN,MAAO,GAAEJ,QAAS,EAA/C,EAAkD;AAChDpF,QAAAA,GAAG,CAAE,QAAO8F,OAAO,CAACN,MAAO,GAAEJ,QAAS,mBAAnC,CAAH,CADgD,CAEhD;AACA;;AACA;AACD,OA1BmC,CA4BpC;;;AACApF,MAAAA,GAAG,CAAE,kBAAiB8F,OAAO,CAACN,MAAO,EAAlC,CAAH;AACA,YAAMS,KAAK,SAASzF,OAAO,CAAC0C,IAAR,CAAaC,MAAb,CAAoB+C,GAApB,CAAwB1C,IAAI,CAACqB,IAA7B,CAApB;AACA,YAAMxD,IAAI,GAAG1B,KAAK,CAACwG,MAAN,CAAaF,KAAb,CAAb,CA/BoC,CAiCpC;;AACA,UAAI,CAACV,IAAI,CAACM,CAAC,GAAG,CAAL,CAAT,EAAkB;AAChB7F,QAAAA,GAAG,CAAE,uBAAsB8F,OAAO,CAACN,MAAO,EAAvC,CAAH;AAEA,cAAM7B,iBAAiB,CAACtC,IAAI,CAACc,KAAN,EAAayB,UAAb,EAAyBkC,OAAO,CAACpF,MAAjC,EAAyCgE,QAAQ,CAACoB,OAAO,CAACN,MAAT,EAAiB,EAAjB,CAAjD,CAAvB;AACA,cAAMT,QAAQ,SAASnB,UAAU,CAAC0B,oBAAX,CAAgCF,QAAhC,CAAvB,CAJgB,CAMhB;;AACAG,QAAAA,IAAI,CAACG,IAAL,CAAU;AACRhF,UAAAA,MAAM,EAAEqE,QAAQ,CAACrE,MADT;AAER8E,UAAAA,MAAM,EAAEV,QAAQ,CAACC,QAAQ,CAACN,GAAV,CAFR;AAGRpD,UAAAA,IAAI,EAAEA;AAHE,SAAV;AAMA;AACD;;AAED,YAAM+E,WAAW,GAAGb,IAAI,CAACM,CAAC,GAAG,CAAL,CAAxB,CAlDoC,CAoDpC;;AACA,YAAM3B,oBAAoB,CAAC7C,IAAI,CAACc,KAAN,EAAaiE,WAAW,CAAC1F,MAAzB,EAAiCkD,UAAjC,CAA1B;AAEAwC,MAAAA,WAAW,CAAC/E,IAAZ,GAAmBA,IAAnB;AACD;;AAED,UAAMuC,UAAU,CAACR,GAAX,CAAegC,QAAf,EAAyB,IAAzB,CAAN;AAEAG,IAAAA,IAAI,CAACnE,OAAL;AAEA,WAAO;AACLwC,MAAAA,UADK;AAEL2B,MAAAA;AAFK,KAAP;AAID,GA7FiB;;AAAA,kBAAZJ,YAAY;AAAA;AAAA;AAAA,GAAlB;AA+FA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACA,MAAMkB,WAAW;AAAA,gCAAG,WAAO7F,OAAP,EAAgB8F,QAAhB,EAA0B3F,OAAO,GAAG,EAApC,EAA2C;AAC7D,UAAM4F,KAAK,GAAG,IAAIxG,UAAJ,CAAe;AAC3ByG,MAAAA,IAAI,EAAE,IADqB;AAE3BjF,MAAAA,GAAG,EAAE,IAFsB;AAG3BX,MAAAA,MAAM,EAAE6F,SAHmB;AAI3BC,MAAAA,SAAS,EAAED,SAJgB;AAK3BlB,MAAAA,IAAI,EAAE,EALqB;AAM3BoB,MAAAA,KAAK,EAAE,IANoB;AAO3BC,MAAAA,IAAI,EAAE,KAPqB;AAQ3B/E,MAAAA,KAAK,EAAElB,OAAO,CAACkB,KARY;AAS3BD,MAAAA,IAAI,EAAEjB,OAAO,CAACiB;AATa,KAAf,EAUXjB,OAVW,CAAd;;AAYA,SAAK,IAAIkF,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGS,QAAQ,CAAC5C,MAA7B,EAAqCmC,CAAC,EAAtC,EAA0C;AACxC,YAAMU,KAAK,CAACM,OAAN,CAAczD,GAAd,CAAkBkD,QAAQ,CAACT,CAAD,CAAR,CAAYiB,IAA9B,EAAoC;AACxCzD,QAAAA,IAAI,EAAEiD,QAAQ,CAACT,CAAD,CAAR,CAAYxC,IADsB;AAExCR,QAAAA,GAAG,EAAEyD,QAAQ,CAACT,CAAD,CAAR,CAAYhD;AAFuB,OAApC,CAAN;AAID;;AAED,UAAMkE,GAAG,SAAS7G,IAAI,CAACqG,KAAK,CAACtD,KAAN,CAAYzC,OAAO,CAAC0C,IAAR,CAAaC,MAAzB,CAAD,CAAtB;;AAEA,QAAI,CAAC4D,GAAL,EAAU;AACR,YAAM,IAAIjG,KAAJ,CAAU,kCAAV,CAAN;AACD;;AAED,WAAOiG,GAAP;AACD,GA3BgB;;AAAA,kBAAXV,WAAW;AAAA;AAAA;AAAA,GAAjB;;AA6BAW,MAAM,CAACC,OAAP,GAAiB;AACf9B,EAAAA,YADe;AAEf5E,EAAAA,mBAFe;AAGfoD,EAAAA,iBAHe;AAIfQ,EAAAA,wBAJe;AAKfD,EAAAA,oBALe;AAMfY,EAAAA,QANe;AAOfuB,EAAAA;AAPe,CAAjB","sourcesContent":["'use strict'\n\nconst dagPb = require('@ipld/dag-pb')\nconst {\n  Bucket,\n  createHAMT\n} = require('hamt-sharding')\nconst DirSharded = require('./dir-sharded')\nconst log = require('debug')('ipfs:mfs:core:utils:hamt-utils')\nconst { UnixFS } = require('ipfs-unixfs')\nconst last = require('it-last')\nconst { CID } = require('multiformats/cid')\nconst {\n  hamtHashCode,\n  hamtHashFn,\n  hamtBucketBits\n} = require('./hamt-constants')\n\n/**\n * @typedef {import('multiformats/cid').CIDVersion} CIDVersion\n * @typedef {import('ipfs-unixfs').Mtime} Mtime\n * @typedef {import('../').MfsContext} MfsContext\n * @typedef {import('@ipld/dag-pb').PBNode} PBNode\n * @typedef {import('@ipld/dag-pb').PBLink} PBLink\n */\n\n/**\n * @param {MfsContext} context\n * @param {PBLink[]} links\n * @param {Bucket<any>} bucket\n * @param {object} options\n * @param {PBNode} options.parent\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n * @param {string} options.hashAlg\n */\nconst updateHamtDirectory = async (context, links, bucket, options) => {\n  if (!options.parent.Data) {\n    throw new Error('Could not update HAMT directory because parent had no data')\n  }\n\n  // update parent with new bit field\n  const data = Uint8Array.from(bucket._children.bitField().reverse())\n  const node = UnixFS.unmarshal(options.parent.Data)\n  const dir = new UnixFS({\n    type: 'hamt-sharded-directory',\n    data,\n    fanout: bucket.tableSize(),\n    hashType: hamtHashCode,\n    mode: node.mode,\n    mtime: node.mtime\n  })\n\n  const hasher = await context.hashers.getHasher(options.hashAlg)\n  const parent = {\n    Data: dir.marshal(),\n    Links: links.sort((a, b) => (a.Name || '').localeCompare(b.Name || ''))\n  }\n  const buf = dagPb.encode(parent)\n  const hash = await hasher.digest(buf)\n  const cid = CID.create(options.cidVersion, dagPb.code, hash)\n\n  if (options.flush) {\n    await context.repo.blocks.put(cid, buf)\n  }\n\n  return {\n    node: parent,\n    cid,\n    size: links.reduce((sum, link) => sum + (link.Tsize || 0), buf.length)\n  }\n}\n\n/**\n * @param {PBLink[]} links\n * @param {Bucket<any>} rootBucket\n * @param {Bucket<any>} parentBucket\n * @param {number} positionAtParent\n */\nconst recreateHamtLevel = async (links, rootBucket, parentBucket, positionAtParent) => {\n  // recreate this level of the HAMT\n  const bucket = new Bucket({\n    hash: rootBucket._options.hash,\n    bits: rootBucket._options.bits\n  }, parentBucket, positionAtParent)\n  parentBucket._putObjectAt(positionAtParent, bucket)\n\n  await addLinksToHamtBucket(links, bucket, rootBucket)\n\n  return bucket\n}\n\n/**\n * @param {PBLink[]} links\n */\nconst recreateInitialHamtLevel = async (links) => {\n  const bucket = createHAMT({\n    hashFn: hamtHashFn,\n    bits: hamtBucketBits\n  })\n\n  await addLinksToHamtBucket(links, bucket, bucket)\n\n  return bucket\n}\n\n/**\n * @param {PBLink[]} links\n * @param {Bucket<any>} bucket\n * @param {Bucket<any>} rootBucket\n */\nconst addLinksToHamtBucket = async (links, bucket, rootBucket) => {\n  await Promise.all(\n    links.map(link => {\n      const linkName = (link.Name || '')\n\n      if (linkName.length === 2) {\n        const pos = parseInt(linkName, 16)\n\n        bucket._putObjectAt(pos, new Bucket({\n          hash: rootBucket._options.hash,\n          bits: rootBucket._options.bits\n        }, bucket, pos))\n\n        return Promise.resolve()\n      }\n\n      return rootBucket.put(linkName.substring(2), {\n        size: link.Tsize,\n        cid: link.Hash\n      })\n    })\n  )\n}\n\n/**\n * @param {number} position\n */\nconst toPrefix = (position) => {\n  return position\n    .toString(16)\n    .toUpperCase()\n    .padStart(2, '0')\n    .substring(0, 2)\n}\n\n/**\n * @param {MfsContext} context\n * @param {string} fileName\n * @param {PBNode} rootNode\n */\nconst generatePath = async (context, fileName, rootNode) => {\n  // start at the root bucket and descend, loading nodes as we go\n  const rootBucket = await recreateInitialHamtLevel(rootNode.Links)\n  const position = await rootBucket._findNewBucketAndPos(fileName)\n\n  // the path to the root bucket\n  /** @type {{ bucket: Bucket<any>, prefix: string, node?: PBNode }[]} */\n  const path = [{\n    bucket: position.bucket,\n    prefix: toPrefix(position.pos)\n  }]\n  let currentBucket = position.bucket\n\n  while (currentBucket !== rootBucket) {\n    path.push({\n      bucket: currentBucket,\n      prefix: toPrefix(currentBucket._posAtParent)\n    })\n\n    // @ts-ignore - only the root bucket's parent will be undefined\n    currentBucket = currentBucket._parent\n  }\n\n  path.reverse()\n  path[0].node = rootNode\n\n  // load PbNode for each path segment\n  for (let i = 0; i < path.length; i++) {\n    const segment = path[i]\n\n    if (!segment.node) {\n      throw new Error('Could not generate HAMT path')\n    }\n\n    // find prefix in links\n    const link = segment.node.Links\n      .filter(link => (link.Name || '').substring(0, 2) === segment.prefix)\n      .pop()\n\n    // entry was not in shard\n    if (!link) {\n      // reached bottom of tree, file will be added to the current bucket\n      log(`Link ${segment.prefix}${fileName} will be added`)\n      // return path\n      continue\n    }\n\n    // found entry\n    if (link.Name === `${segment.prefix}${fileName}`) {\n      log(`Link ${segment.prefix}${fileName} will be replaced`)\n      // file already existed, file will be added to the current bucket\n      // return path\n      continue\n    }\n\n    // found subshard\n    log(`Found subshard ${segment.prefix}`)\n    const block = await context.repo.blocks.get(link.Hash)\n    const node = dagPb.decode(block)\n\n    // subshard hasn't been loaded, descend to the next level of the HAMT\n    if (!path[i + 1]) {\n      log(`Loaded new subshard ${segment.prefix}`)\n\n      await recreateHamtLevel(node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16))\n      const position = await rootBucket._findNewBucketAndPos(fileName)\n\n      // i--\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: node\n      })\n\n      continue\n    }\n\n    const nextSegment = path[i + 1]\n\n    // add intermediate links to bucket\n    await addLinksToHamtBucket(node.Links, nextSegment.bucket, rootBucket)\n\n    nextSegment.node = node\n  }\n\n  await rootBucket.put(fileName, true)\n\n  path.reverse()\n\n  return {\n    rootBucket,\n    path\n  }\n}\n\n/**\n * @param {MfsContext} context\n * @param {{ name: string, size: number, cid: CID }[]} contents\n * @param {object} [options]\n * @param {Mtime} [options.mtime]\n * @param {number} [options.mode]\n */\nconst createShard = async (context, contents, options = {}) => {\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: undefined,\n    parentKey: undefined,\n    path: '',\n    dirty: true,\n    flat: false,\n    mtime: options.mtime,\n    mode: options.mode\n  }, options)\n\n  for (let i = 0; i < contents.length; i++) {\n    await shard._bucket.put(contents[i].name, {\n      size: contents[i].size,\n      cid: contents[i].cid\n    })\n  }\n\n  const res = await last(shard.flush(context.repo.blocks))\n\n  if (!res) {\n    throw new Error('Flushing shard yielded no result')\n  }\n\n  return res\n}\n\nmodule.exports = {\n  generatePath,\n  updateHamtDirectory,\n  recreateHamtLevel,\n  recreateInitialHamtLevel,\n  addLinksToHamtBucket,\n  toPrefix,\n  createShard\n}\n"]},"metadata":{},"sourceType":"script"}